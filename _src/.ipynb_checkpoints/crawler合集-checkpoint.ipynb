{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从web抓取信息\n",
    "\n",
    "起源于《Python编程快速上手——让繁琐工作自动化》中的第11章“从web抓取信息”，也有自己生发的及其他书的内容。\n",
    "\n",
    "几个重要的库：  \n",
    "\n",
    "- 1. `webbrowser`\n",
    "- 2. `requests`\n",
    "- 3. `BeautifulSoup`  \n",
    "- 4. `selenium`\n",
    "- 5. `scrapy`\n",
    "\n",
    "另外还有`urllib`，但AI Sweigart 说让我忘了这个库，意思是很不好用。\n",
    "\n",
    "- js载入的动态网页内容\n",
    "\n",
    "---\n",
    "附录：关于HTML\n",
    "\n",
    "**苹果系统中，command+option+I，可以打开或关闭开发者工具，和Windows上的F12是一样的。**  \n",
    "\n",
    "作者建议，**不要用正则表达式来解析HTML**。例如昨天遇到的将class写在a标签中间的那种，对于html来说仍然有效，用正则来预估所有的情况则会非常繁琐。专门用来解析html的模块，例如beautifulsoup，将更不容易出错。[html - RegEx match open tags except XHTML self-contained tags - Stack Overflow](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454)\n",
    "\n",
    "---\n",
    "\n",
    "一张图：  \n",
    "\n",
    "![](https://user-gold-cdn.xitu.io/2019/8/22/16cb96d421b52ad9?imageslim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython输出各行结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  `webbrowser`模块\n",
    "\n",
    "`webbrowser`这个模块，可以直接打开网址。\n",
    "\n",
    "先生成网址，再用webbrowser打开，适合网址有规律的情况。哪些情况适用于生成网址再打开检查的情况？\n",
    "\n",
    "- 动态网址  \n",
    "\n",
    "    - 查询类：内容参数组成新网址，如Google Map\n",
    "    - 解析类：从某段文字中解析出需要的新网址\n",
    "    \n",
    "- 页面内容确认\n",
    "\n",
    "    - 编号类：某种无序数列组成新网址，如小说网站晋江\n",
    "    - 已经获得，手动打开麻烦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 打开小说页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "for i in range(3000011, 3000020):\n",
    "    url = 'http://www.jjwxc.net/onebook.php?novelid=' + str(i)\n",
    "    webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 打开Google地图上单个城市"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import webbrowser\n",
    "import re\n",
    "\n",
    "def mapit(address):\n",
    "    address = re.compile(r'\\s+').sub('+',address) # 这里“+”之前不需要转义符\\\n",
    "    url = 'http://www.google.com/maps/place/' + address\n",
    "    return webbrowser.open(url)\n",
    "\n",
    "address = 'Wuxi,   Jiangsu,   China'    \n",
    "mapit(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 批量打开Google地图的城市群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "def mapcities(cities):\n",
    "    address = []\n",
    "    for city in cities:\n",
    "        address.append(city + ', Jiangsu, China')\n",
    "    for a in address:\n",
    "        a = re.compile(r'\\s+').sub('+',a)\n",
    "        url = 'http://www.google.com/maps/place/' + a\n",
    "        webbrowser.open(url)\n",
    "        \n",
    "cities = ['Wuxi', 'Suzhou', 'Xuzhou', 'Zhengjiang', 'Taizhou']\n",
    "mapcities(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 批量打开简书笔记\n",
    "\n",
    "复制下面这段网址（从workflowy来），用下面的程序批量打开：\n",
    "\n",
    "    [每天一本书 -《思考线》](https://www.jianshu.com/p/ee5e1c32f97d)\n",
    "\t[创意变为现实的最佳方法——《思考线》读后感](https://www.jianshu.com/p/c2132f7e02ae)\n",
    "\t[读思考线 ](https://www.jianshu.com/p/52e2e4dbb08c)\n",
    "\t[极具说服力的书（思考线：让你的创意变为现实的最佳方法](https://book.douban.com/review/7747085/)\n",
    "\t[思考线·思维导图.png (3104×1802)](https://upload-images.jianshu.io/upload_images/14183687-4b46af1a5294edfd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser, pyperclip, re\n",
    "\n",
    "mdUrls = pyperclip.paste().replace('\\t','').split('\\n')\n",
    "urlRegex = re.compile(r'\\((http.*)\\)')\n",
    "for mdUrl in mdUrls:\n",
    "    url = urlRegex.search(mdUrl).group(1)\n",
    "    webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `requests`模块\n",
    "\n",
    "requests文档地址：[Requests: HTTP for Humans™ — Requests 2.21.0 documentation](https://requests.readthedocs.io/en/master/)\n",
    "\n",
    "`requests.get(url)`:  \n",
    "\n",
    "- 类型是`requests.models.Response`\n",
    "- 参数text\n",
    "- 参数headers是类似于字典（字典有`dict.get(key)`返回value的语法）的结构：`requests.structures.CaseInsensitiveDict`，它的键不区分大小写。真正的字典键是区分大小写的。\n",
    "- 参数status_code\n",
    "- 参数encoding\n",
    "- 方法raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ISO-8859-1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "179378"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ufeffThe Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\\r\\n\\r\\n\\r\\n*******************************************************************\\r\\nTHIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A\\r\\nTIME WHEN PROOFING METHODS AND TOO\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'text/plain; charset=utf-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Server': 'AliyunOSS', 'Date': 'Fri, 01 Nov 2019 01:39:49 GMT', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'x-oss-request-id': '5DBB8CE55C74183036984F90', 'Last-Modified': 'Thu, 31 Oct 2019 02:27:20 GMT', 'x-oss-object-type': 'Normal', 'x-oss-hash-crc64ecma': '4434422554274640270', 'x-oss-storage-class': 'Standard', 'Content-MD5': 'l2yxdfF3i/9yvmsmNVa0SA==', 'x-oss-server-time': '18', 'Content-Encoding': 'gzip'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-21c72c8a0df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0murl3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://inventwithpython.com/page_that_does_not_exist'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mres3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mres3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import requests\n",
    "import chardet\n",
    "\n",
    "url0 = 'http://www.gutenberg.org/cache/epub/1112/pg1112.txt'\n",
    "url1 = 'http://example.webscraping.com'\n",
    "url2 = 'http://www.engine3d.com'\n",
    "\n",
    "res0 = requests.get(url0)\n",
    "res1 = requests.get(url1)\n",
    "headers = {'user-agent': 'my-app/0.0.1'}\n",
    "res2 = requests.get(url2, headers = headers)\n",
    "\n",
    "# res的基本情况\n",
    "type(res0)\n",
    "res0.status_code == requests.codes.ok\n",
    "res2.encoding\n",
    "\n",
    "# 看res的文本\n",
    "len(res0.text)\n",
    "res0.text[:250]\n",
    "\n",
    "# 看res的headers\n",
    "res0.headers.get('content-type')\n",
    "res2.headers\n",
    "res2.headers.get('user-agent')\n",
    "\n",
    "# 不存在的网址，看res反应\n",
    "url3 = 'http://inventwithpython.com/page_that_does_not_exist'\n",
    "res3 = requests.get(url3)\n",
    "res3.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 能运行起来的第一段程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url0的下载结果： \n",
      "\n",
      "Webpage is download successfully.\n",
      "The beginning texts here:  \n",
      "\n",
      "\n",
      "﻿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\n",
      "\n",
      "\n",
      "*******************************************************************\n",
      "THIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES\n",
      "url3的下载结果： \n",
      "\n",
      "There was a problem: \n",
      "404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist\n"
     ]
    }
   ],
   "source": [
    "def download(url):\n",
    "    res = requests.get(url)\n",
    "    try:\n",
    "        res.raise_for_status()\n",
    "        html = res.text\n",
    "        print('Webpage is download successfully.'+'\\n'+'The beginning texts here:  '+'\\n\\n')\n",
    "        print(html[:200])\n",
    "    except Exception as exc :\n",
    "        print('There was a problem: \\n%s' %exc)\n",
    "print('url0的下载结果： \\n')\n",
    "download(url0)\n",
    "print('url3的下载结果： \\n')\n",
    "download(url3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 保存文件到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "79380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "res = requests.get(url0)\n",
    "res.raise_for_status()\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'RomeoAndJuliet.txt')\n",
    "playFile = open(path, 'wb')\n",
    "for chunk in res.iter_content(100000):\n",
    "    playFile.write(chunk)\n",
    "playFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 源代码的编码问题\n",
    "\n",
    "关于Unicode编码的知识：[The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) – Joel on Software](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n",
    "\n",
    "```\n",
    "encode_type = chardet.detect(html)\n",
    "html = html.decode(encode_type['encoding'])\n",
    "```\n",
    "\n",
    "这里不是靠这两句解决问题的。已经测试过，html是str类型的。\n",
    "\n",
    "用wb模式打开文件，写入内容是 `res.iter_content(100000)`  \n",
    "作者说，使用iter_content是为了确保requests模块即使在**下载巨大**的文件时也**不会消耗太多**内存。\n",
    "\n",
    "**The chunk size is the number of bytes it should read into memory. **\n",
    "\n",
    "最后靠这篇[Python爬虫及存入txt中文编码错误的解决（一） - WANGZHUCHEN的博客 - CSDN博客](https://blog.csdn.net/WANGZHUCHEN/article/details/80033073)解决了中文网页乱码问题。\n",
    "\n",
    "```\n",
    "res.encoding = res.apparent_encoding\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1536"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _*_coding:utf-8_*_\n",
    "import requests, bs4, os, lxml, chardet\n",
    "\n",
    "url = 'https://www.114zw.la/book/1425/8236687.html'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "res.encoding = res.apparent_encoding\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "content = soup.select('#htmlContent')[0].text\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'jinshi.txt')\n",
    "with open(path, 'w') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换了一个新网页，没有这个问题，所以这句话`res.encoding = res.apparent_encoding`也可以不用。  \n",
    "看了一下，两个网页的charse都是写的gbk。所以也不知道是哪里的设置导致这种编码保护。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3224"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _*_coding:utf-8_*_\n",
    "import requests, bs4, os, lxml, chardet\n",
    "\n",
    "url = 'https://m.lewenxiaoshuo.com/books/tufeigonglve/12880240.html'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "# res.encoding = res.apparent_encoding\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "content = soup.select('#content')[0].text\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'tufei.txt')\n",
    "with open(path, 'w') as file:\n",
    "    file.write(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `BeautifulSoup`模块\n",
    "\n",
    "[BeautifulSoup高级应用 之 CSS selectors /CSS 选择器 - Winterto1990的博客 - CSDN博客](https://blog.csdn.net/Winterto1990/article/details/47808949)\n",
    "\n",
    "soup的两类来源：\n",
    "\n",
    "- 1. 从网页获取\n",
    "- 2. 从本地获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "# soup的两类来源：\n",
    "# 1. 从网页获取\n",
    "url = 'https://www.tripadvisor.cn/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "example1Soup = bs4.BeautifulSoup(res.text)\n",
    "type(example1Soup)\n",
    "\n",
    "# 2. 从本地获取\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'NoStarch.html')\n",
    "example2File = open(path)\n",
    "example2Soup = bs4.BeautifulSoup(example2File.read()) #.read()加了没加没区别。\n",
    "type(example2Soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>TripAdvisor(猫途鹰) - 全球旅游点评,酒店/景点/餐厅,真实旅客评论</title>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<img alt=\"TripAdvisor(猫途鹰)\" class=\"brand-header-Logo__resizeImg--15ZcW\" src=\"https://cc.ddcdn.com/img2/langs/zh_CN/branding/rebrand/TA_logo_primary.svg\"/>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<img alt=\"TripAdvisor(猫途鹰)\" class=\"brand-header-Logo__resizeImg--15ZcW\" src=\"https://cc.ddcdn.com/img2/langs/zh_CN/branding/rebrand/TA_logo_primary.svg\"/>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ntripSoup.select(\\'input[type=\"radio\"]\\')\\ntripSoup.select_one(\\'input[type=\"radio\"]\\')\\ntripSoup.select(\\'#popularDestinations > div.section > ul.regionContent > li.active > ul > li:nth-child(1) > a > span.thumbCrop > img\\')\\nimages = tripSoup.select(\\'#popularDestinations > div.section > ul.regionContent > li.active > ul > li > a > span.thumbCrop > img\\')\\ntitles = tripSoup.select(\\'#popularDestinations > div.section > ul.regionContent > li.active > ul > li > div.title\\')\\ninfo = []\\nfor title,image in zip(titles, images):\\n    data = {\\n            \\'title\\':((title.get_text()).replace(\\'\\n\\',\\'\\')).replace(\\'游记指南\\',\\'\\'),\\n            \\'image\\':image.get(\\'src\\')\\n        }\\n    info.append(data)\\ninfo\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['height:100%', 'width:100%', 'background-size:cover', 'background-image:none']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lxml解析 \n",
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "\n",
    "url = 'https://www.tripadvisor.cn/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "html = res.text\n",
    "tripSoup = bs4.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "tripSoup.select('title')\n",
    "tripSoup.select_one('a>img')\n",
    "tripSoup.select_one('a img')\n",
    "# 网页变了，以下代码失效。\n",
    "'''\n",
    "tripSoup.select('input[type=\"radio\"]')\n",
    "tripSoup.select_one('input[type=\"radio\"]')\n",
    "tripSoup.select('#popularDestinations > div.section > ul.regionContent > li.active > ul > li:nth-child(1) > a > span.thumbCrop > img')\n",
    "images = tripSoup.select('#popularDestinations > div.section > ul.regionContent > li.active > ul > li > a > span.thumbCrop > img')\n",
    "titles = tripSoup.select('#popularDestinations > div.section > ul.regionContent > li.active > ul > li > div.title')\n",
    "info = []\n",
    "for title,image in zip(titles, images):\n",
    "    data = {\n",
    "            'title':((title.get_text()).replace('\\n','')).replace('游记指南',''),\n",
    "            'image':image.get('src')\n",
    "        }\n",
    "    info.append(data)\n",
    "info\n",
    "'''\n",
    "# 不知道为什么读不到背景图像的url\n",
    "tripSoup.select('a>div>ul>li>div')[0]['style'].split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Prashanth!\n",
      "https://www.joelonsoftware.com/2019/09/24/announcing-stack-overflows-new-ceo/\n",
      "The next CEO of Stack Overflow\n",
      "https://www.joelonsoftware.com/2019/03/28/the-next-ceo-of-stack-overflow/\n",
      "Things You Should Never Do, Part I\n",
      "Strategy Letter I: Ben and Jerry’s vs. Amazon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://i1.wp.com/www.joelonsoftware.com/wp-content/uploads/2016/12/Pong.png?w=730&ssl=1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://www.joelonsoftware.com/'\n",
    "res = requests.get(url)\n",
    "htmlSoup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "import pprint\n",
    "for i in range(2):\n",
    "    # pprint.pprint(htmlSoup.select('div>p>span>a')[i].attrs)\n",
    "    # print(htmlSoup.select('div>p>span>a')[i].get('href'))\n",
    "    # pprint.pprint(htmlSoup.select('header>h2>a')[i].attrs)\n",
    "    print(htmlSoup.select('header>h2>a')[i].text)\n",
    "    print(htmlSoup.select('header>h2>a')[i].get('href'))\n",
    "for i in range(2):\n",
    "    print(htmlSoup.select('div>ul>li>a')[i].text)\n",
    "    \n",
    "htmlSoup.select_one('img')['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>CSDN博客-专业IT技术发表平台</title>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a href=\"/\">推荐</a>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\t\\t\\t\\t\\tCSDN产品公告第2期：博客支持视频、专栏文章拖拽排序、APP霸王课来袭……\\t\\t\\t\\t\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://blog.csdn.net' \n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "soup.select('title')\n",
    "# soup.select('div nth-of-type(0)') 在ipython中不好用\n",
    "# soup.select('body a')[:3]\n",
    "# soup.select('div>ul>li.active')\n",
    "# soup.select('.carousel-caption, p.name')\n",
    "soup.select('a[href]')[0]\n",
    "soup.select(\"a[href$='102605809']\")[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 下载古腾堡中文书\n",
    "\n",
    "突然想下载古腾堡的书下来。挑选中文的试试吧。  \n",
    "\n",
    "【下载】\n",
    "1. 通过主页面提取出txt的url\n",
    "2. 使用上面的功能下载这些。\n",
    "\n",
    "【繁体转简体】\n",
    "找到对应的库 \n",
    "\n",
    "> 不需要什么安装方法，只需要把这两个文件下载下来，保存到与代码同一目录下即可\n",
    "```\n",
    "https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py  \n",
    "https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py\n",
    "```\n",
    "\n",
    "#### 3.2.1 下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载txt\n",
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "url = 'https://www.gutenberg.org/browse/languages/zh'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "fictionSoup = bs4.BeautifulSoup(res.text)\n",
    "fictionList = fictionSoup.select(\"li.pgdbetext a[href^='/ebooks']\")\n",
    "for i in range(len(fictionList)):\n",
    "    fileName = fictionList[i].get_text()    \n",
    "    if '/' in fileName:\n",
    "        fileName = fileName.replace('/', ' ')\n",
    "    if '\\\\' in fileName:\n",
    "        fileName = fileName.replace('\\\\', ' ')\n",
    "    file = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', fileName +'.txt')\n",
    "    if os.path.exists(file):\n",
    "        fileName = fileName + '_new'\n",
    "        file = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', fileName +'.txt')\n",
    "    fictionID = (fictionList[i].get('href')).split('/')[2]\n",
    "    fictionUrl = 'https://www.gutenberg.org/files/' + fictionID + '/' + fictionID + '-0.txt'\n",
    "    with open(file ,'wb') as fictionFile:\n",
    "        resFiction = requests.get(fictionUrl)\n",
    "        for chunk in resFiction.iter_content(100000):\n",
    "            fictionFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共下载了475本txt文本格式的电子书。  \n",
    "\n",
    "现在看txt格式的都可以下载，试试其他格式的：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载epub\n",
    "\n",
    "file = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', '玉樓春.epub')\n",
    "url = 'https://www.gutenberg.org/ebooks/25422.epub.noimages?session_id=3c29b07a963878c5cd004f277b6d1d0adb08d623'\n",
    "with open(file ,'wb') as fictionFile:\n",
    "        resFiction = requests.get(url)\n",
    "        for chunk in resFiction.iter_content(100000):\n",
    "            fictionFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 转换简繁体  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玉楼春'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langconv import *\n",
    "\n",
    "sentence = '玉樓春'\n",
    "Converter('zh-hans').convert(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想批量转换，需要：  \n",
    "\n",
    "1. 转换文件名  \n",
    "2. 建立新文件——考虑到有些文件名已经是简体，保险的方法就是再建立一个简体文件夹。  \n",
    "3. 读取文件中的内容\n",
    "4. 转换文件内容  \n",
    "5. 写入新文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langconv import Converter\n",
    "\n",
    "pathFanti = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'fanti')\n",
    "pathJianti = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'jianti')\n",
    "filesFanti = os.listdir(pathFanti) #把这个目录下的所有文件都读出来\n",
    "for fileName in filesFanti:\n",
    "    if fileName.split('.')[-1] != 'txt':\n",
    "        filesFanti.remove(fileName)\n",
    "for fileNameFanti in filesFanti:\n",
    "    fileNameJianti = Converter('zh-hans').convert(fileNameFanti)\n",
    "    with open(os.path.join(pathJianti, fileNameJianti), 'w') as fileJianti:\n",
    "        with open(os.path.join(pathFanti, fileNameFanti)) as fileFanti:\n",
    "            contentFanti = fileFanti.readlines()\n",
    "            for sentenceFanti in contentFanti:\n",
    "                sentenceJianti = Converter('zh-hans').convert(sentenceFanti)\n",
    "                fileJianti.write(sentenceJianti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了一个打不开的，其他都转换成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Google自动查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googling...\n"
     ]
    }
   ],
   "source": [
    "# _*_coding:utf-8_*_\n",
    "import requests, bs4, webbrowser,re\n",
    "\n",
    "def googleit(query):\n",
    "    \n",
    "    #打开查询结果页面\n",
    "    if '' in query:\n",
    "        query = re.compile(r'\\s+').sub('+', query)\n",
    "    url = 'http://www.google.com/search?q=' + query\n",
    "    print('Googling...')\n",
    "    headers = {'User-Agent':'Mozilla/8.0 (compatible; MSIE 8.0; Windows 7)'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    #选择结果页面\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    linkElems = soup.select('.r a')\n",
    "    \n",
    "    #打开前5个页面\n",
    "    numOpen = min(5, len(linkElems))\n",
    "    for i in range(numOpen):\n",
    "        webbrowser.open('http://google.com' + linkElems[i].get('href'))\n",
    "        # print(linkElems[i].get('href'))\n",
    "query = 'python webscraping'\n",
    "googleit(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 下载xkcd漫画\n",
    "\n",
    "这是第一次尝试下载图片。先开始是教材上的程序，后来自己又重新写了一下。\n",
    "\n",
    "#### 3.4.1 标准程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That is wrong. Downloading image is http://www.xkcd.com/2067/asset/challengers_header.png ...\n",
      "Could not find comic image.\n",
      "Could not find comic image.\n",
      "That is wrong. Downloading image is http://www.xkcd.com/1525/bg.png ...\n",
      "Could not find comic image.\n",
      "Could not find comic image.\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import requests, bs4,os\n",
    "\n",
    "url = 'http://xkcd.com'\n",
    "# os.makedirs('xkcd', exist_ok=True)\n",
    "while not url.endswith('#'):\n",
    "    #下载网页\n",
    "    # print('Downloading page %s ...'%url)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    \n",
    "    #寻找漫画\n",
    "    comicElem = soup.select('#comic img')\n",
    "    if comicElem == []:\n",
    "        print('Could not find comic image.')\n",
    "    else:\n",
    "        try:\n",
    "            comicUrl = 'http:'+ comicElem[0].get('src')\n",
    "            # print('Downloading image is %s ...'%comicUrl)\n",
    "            res = requests.get(comicUrl)\n",
    "            res.raise_for_status()  \n",
    "        except:\n",
    "            comicUrl = 'http://www.xkcd.com'+ comicElem[0].get('src')\n",
    "            print('That is wrong. Downloading image is %s ...'%comicUrl)    \n",
    "            res = requests.get(comicUrl)\n",
    "            res.raise_for_status() \n",
    "\n",
    "    #保存漫画\n",
    "    # imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')\n",
    "    imageFile = open(os.path.dirname(os.getcwd()) + '/files/xkcd/' + os.path.basename(comicUrl), 'wb')\n",
    "    for chunk in res.iter_content(100000):\n",
    "        imageFile.write(chunk)\n",
    "    imageFile.close() \n",
    "\n",
    "    #找前一张漫画\n",
    "    prevLink = soup.select('a[rel=\"prev\"]')[0]\n",
    "    url = 'http://xkcd.com' + prevLink.get('href')\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image is http://imgs.xkcd.com/comics/pie_charts.png ...\n"
     ]
    }
   ],
   "source": [
    "url = 'https://xkcd.com/2031/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "comicElem = soup.select('#comic img')\n",
    "try:\n",
    "    comicUrl = 'http:'+ comicElem[0].get('src')\n",
    "    print('Downloading image is %s ...'%comicUrl)\n",
    "    res = requests.get(comicUrl)\n",
    "    res.raise_for_status()  \n",
    "except:\n",
    "    comicUrl = 'http://www.xkcd.com'+ comicElem[0].get('src')\n",
    "    print('That is wrong. Downloading image is %s ...'%comicUrl)    \n",
    "    res = requests.get(comicUrl)\n",
    "    res.raise_for_status() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明白了，这里用的是canvas，不是img。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2031和1813都出现的错误：  \n",
    "\n",
    "SysCallError                              Traceback (most recent call last)\n",
    "\n",
    "SSLError: HTTPSConnectionPool(host='xkcd.com', port=443): Max retries exceeded with url: /1813/ (Caused by SSLError(SSLError(\"bad handshake: SysCallError(-1, 'Unexpected EOF')\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.4.2 重新下载xkcd漫画\n",
    "\n",
    "在知道了漫画作者门罗就是《万物解释者》作者后，决定重新下载xkcd漫画。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "url = 'https://xkcd.com/1'\n",
    "while True:\n",
    "    # 获取当前页面的漫画地址\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    try:\n",
    "        # 当前页面存在漫画的静态图片地址\n",
    "        srcComic = soup.select_one('#comic img').get('src')\n",
    "        urlComic = 'https:'+ srcComic\n",
    "    except:\n",
    "        # 没有漫画地址就直接找下一页\n",
    "        srcPrev = soup.select_one(\".comicNav a[rel='prev']\").get('href')\n",
    "        print(srcPrev)\n",
    "        url = 'https://xkcd.com' + srcPrev\n",
    "        continue\n",
    "    \n",
    "    # 存储当前漫画到本地\n",
    "    fileName = srcComic.split('/')[-1]\n",
    "    filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'xkcd', fileName)\n",
    "    with open(filePath, 'wb') as comicFile:\n",
    "        resComic = requests.get(urlComic)\n",
    "        for chunk in resComic.iter_content(100000):\n",
    "            comicFile.write(chunk)\n",
    "    \n",
    "    # 获取前一页的页面地址\n",
    "    srcPrev = soup.select_one(\".comicNav a[rel='prev']\").get('href')\n",
    "    # 到了第一幅漫画的页面\n",
    "    if srcPrev == '#':\n",
    "        break\n",
    "    print(srcPrev)\n",
    "    url = 'https://xkcd.com' + srcPrev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有问题的：2198, 2067，1663，1608，1525，1416，1350  \n",
    "\n",
    "2198、1663、1608、1525、1416都是游戏，页面有交互，非普通静态图片。1416会放大，嵌入了一个框架网页。1350无内容。\n",
    "\n",
    "2067会放大，其中有链接，也不是普通图片。错误显示为：  \n",
    "\n",
    "```\n",
    "MissingSchema: Invalid URL 'https:/2067/asset/challengers_header.png': No schema supplied. Perhaps you meant http://https:/2067/asset/challengers_header.png?\n",
    "```\n",
    "\n",
    "1052-878之间一次性完成，无意外。874-787一次性完成。691-483一次性完成。446-250一次性完成\n",
    "\n",
    "超时的错误为：  \n",
    "\n",
    "```\n",
    "SSLError: HTTPSConnectionPool(host='xkcd.com', port=443): Max retries exceeded with url: /240/ (Caused by SSLError(SSLError(\"bad handshake: SysCallError(60, 'ETIMEDOUT')\")))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "1. 需要写一个try-except，出了意外，直接找prev按钮继续走下去。 \n",
    "   try-except无法解决2067的问题。\n",
    "2. 对于timeout怎么应对？\n",
    "\n",
    "目前程序的三个问题：  \n",
    "\n",
    "1. try-except无法解决2067的问题  \n",
    "2. timeout无法解决  \n",
    "3. 下载速度太慢，平均每小时只能下载200-300张漫画，全部2200张漫画需要七八个小时才能下完。  \n",
    "\n",
    "问题三可以用多线程来解决。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 单独下载一副漫画\n",
    "\n",
    "url = 'https://xkcd.com/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "srcComic = soup.select_one('#comic img').get('src')\n",
    "urlComic = 'https:'+ srcComic\n",
    "\n",
    "# 存储当前漫画到本地\n",
    "fileName = srcComic.split('/')[-1]\n",
    "filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'xkcd', fileName)\n",
    "with open(filePath, 'wb') as comicFile:\n",
    "    resComic = requests.get(urlComic)\n",
    "    for chunk in resComic.iter_content(100000):\n",
    "        comicFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 多线程下载xkcd漫画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download End.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import threading\n",
    "import os\n",
    "\n",
    "def downloadComic(startComic, endComic):\n",
    "    for comicID in range(startComic, endComic):\n",
    "        # 获取漫画地址\n",
    "        res = requests.get('https://xkcd.com/%s'%str(comicID))\n",
    "        soup = bs4.BeautifulSoup(res.text)\n",
    "        comicElem = soup.select('#comic img')\n",
    "        if comicElem == []:\n",
    "            print('Could not find comic img: %s'%str(comicID))\n",
    "        else:\n",
    "            srcComic = comicElem[0].get('src')\n",
    "            urlComic = 'https:'+ srcComic\n",
    "            # 存储到本地\n",
    "            fileName = str(comicID) + '-' + srcComic.split('/')[-1]\n",
    "            filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'xkcd', fileName)\n",
    "            if os.path.exists(filePath):\n",
    "                continue\n",
    "            with open(filePath, 'wb') as comicFile:\n",
    "                resComic = requests.get(urlComic)\n",
    "                for chunk in resComic.iter_content(100000):\n",
    "                    comicFile.write(chunk)\n",
    "\n",
    "downloadTreads = []\n",
    "for i in range(1, 81, 10):\n",
    "    downloadTread = threading.Thread(target=downloadComic, args=[i, i+10])\n",
    "    downloadTreads.append(downloadTread)\n",
    "    downloadTread.start()\n",
    "for downloadTread in downloadTreads:\n",
    "    downloadTread.join()\n",
    "print('Download End.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了顺利地在所有线程结束后打印‘Download End.’，必须所有线程都没有崩溃才行。timeout一旦出现，这个程序就处于永远结束不了的状态了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 下载极客漫画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, os\n",
    "\n",
    "for i in range(1,6):\n",
    "    url = 'https://linux.cn/talk/comic/index.php?page=' + str(i)\n",
    "    headers = {'User-Agent':'Mozilla/8.0 (compatible; MSIE 8.0; Windows 7)'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    for s in soup.select('h2 span[class=\"title\"] a'):\n",
    "        comicUrl = s.get('href')\n",
    "        comicRes = requests.get(comicUrl, headers=headers)\n",
    "        comicRes.raise_for_status()\n",
    "        comicSoup = bs4.BeautifulSoup(comicRes.text)\n",
    "        comicImageUrl = comicSoup.select('#article_content img')[0].get('src')\n",
    "        comicImageRes = requests.get(comicImageUrl, headers=headers)\n",
    "        imageFile = open(os.path.dirname(os.getcwd()) + '/files/jkmh/' + os.path.basename(comicImageUrl), 'wb')\n",
    "        for chunk in comicImageRes.iter_content(100000):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 玩豆瓣\n",
    "\n",
    "#### 3.6.1 豆瓣top250图书\n",
    "\n",
    "豆瓣有个top250的图书榜单，读取其中书名、作者和评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '红楼梦', 'author': '曹雪芹', 'score': '9.6'}\n",
      "{'title': '海贼王:ONEPIECE', 'author': '尾田荣一郎', 'score': '9.5'}\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml, re\n",
    "urls = []\n",
    "for i in range(0,250,25):\n",
    "    urls.append('https://book.douban.com/top250?start=' + str(i))\n",
    "info = []\n",
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    html = res.text\n",
    "    doubanBookSoup = bs4.BeautifulSoup(html, 'lxml')\n",
    "    titles = doubanBookSoup.select('a[title]')\n",
    "    scores = doubanBookSoup.select('span[class=\"rating_nums\"]')\n",
    "    authors = doubanBookSoup.select('p.pl')\n",
    "    for title, author, score in zip(titles, authors, scores):\n",
    "        data = {\n",
    "            'title':((title.get_text()).replace('\\n','')).replace(' ','') ,\n",
    "            'author':re.compile(r'(\\[\\w+\\])?(\\w)+(·)?(\\w+)(·\\w+)?').search(author.get_text()).group(),\n",
    "            'score':score.get_text()\n",
    "        }\n",
    "        info.append(data)\n",
    "\n",
    "for i in info:\n",
    "    if 9.5<=float(i['score'])<9.7:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': '红楼梦', 'Rating': '9.6', 'Author': '[清] 曹雪芹 著 '}\n",
      "{'Title': '海贼王', 'Rating': '9.5', 'Author': '尾田荣一郎 '}\n"
     ]
    }
   ],
   "source": [
    "# 2019.11.01重新写一遍\n",
    "import requests, bs4, lxml, os\n",
    "\n",
    "topBooks = []\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "}\n",
    "for pageNum in range(0,250,25):\n",
    "    url = 'https://book.douban.com/top250?start=' + str(pageNum)\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    for i in range(25):\n",
    "        info = soup.select('p.pl')[i].text\n",
    "        data = {\n",
    "            'Title':soup.select('.pl2 a')[i]['title'],\n",
    "            'Rating':soup.select('.rating_nums')[i].text,    \n",
    "            'Author': info.split('/')[0]\n",
    "        }\n",
    "        topBooks.append(data)\n",
    "for book in topBooks:\n",
    "    if float(book['Rating'])>=9.5:\n",
    "        print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 豆瓣标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 单标签\n",
    "import requests, bs4, lxml\n",
    "import os, openpyxl, re, threading\n",
    "\n",
    "def saveContent(tag):\n",
    "    # 建立并保存初始的Excel表\n",
    "    wb = openpyxl.Workbook()\n",
    "    sheetTag = wb.create_sheet()\n",
    "    sheetTag.title = tag\n",
    "    sheetTag['A1'], sheetTag['B1'], sheetTag['C1'] = 'Label', 'Title', 'Author'\n",
    "    sheetTag['D1'], sheetTag['E1'], sheetTag['F1'] = 'Rating', 'Comments', 'Link' \n",
    "    path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'douban', 'tagsEdu.xlsx')\n",
    "    wb.save(path)\n",
    "    wb.close()\n",
    "    \n",
    "    # 初始url\n",
    "    urlTag = 'https://book.douban.com/tag/%E6%95%99%E8%82%B2'\n",
    "\n",
    "    # 读取数据\n",
    "    total = 0\n",
    "    while True:\n",
    "        resTag = requests.get(urlTag)\n",
    "        resTag.raise_for_status()\n",
    "        soupTag = bs4.BeautifulSoup(resTag.text, 'lxml')\n",
    "        num = len(soupTag.select('h2 a'))\n",
    "        if num == 0:\n",
    "            break\n",
    "        for i in range(num):\n",
    "            sheetTag['A'+str(i+2+total)] = soupTag.select('h1')[0].text.split(': ')[1]\n",
    "            # print(soupTag.select('h2 a')[i].get('title'))\n",
    "            sheetTag['B'+str(i+2+total)] = soupTag.select('h2 a')[i].get('title')\n",
    "            sheetTag['C'+str(i+2+total)] = re.compile(r'\\s+').sub('', soupTag.select('div.pub')[i].text).split('/')[0]\n",
    "            sheetTag['F'+str(i+2+total)] = soupTag.select('h2 a')[i].get('href') \n",
    "            if '少于10人评价' in soupTag.select('.clearfix .pl')[i].text or '无人评价' in soupTag.select('.clearfix .pl')[i].text:\n",
    "                sheetTag['D'+str(i+2+total)] = ''\n",
    "                sheetTag['E'+str(i+2+total)] = 0\n",
    "            else:\n",
    "                sheetTag['D'+str(i+2+total)] = float(soupTag.select('.info .clearfix')[i].select('.rating_nums')[0].text)\n",
    "                sheetTag['E'+str(i+2+total)] = int(re.compile(r'\\d+').search(soupTag.select('.clearfix .pl')[i].text).group(0))\n",
    "        total = total + num\n",
    "        urlTag = 'https://book.douban.com' + soupTag.select('.next link')[0].get('href')\n",
    "    # 保存数据\n",
    "    wb.save(path)\n",
    "    wb.close()\n",
    "saveContent('教育')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n"
     ]
    }
   ],
   "source": [
    "# 重写多标签，使用User-Agent，多线程，补充无下一页情况应对\n",
    "import requests, bs4, lxml, os, openpyxl, re, threading\n",
    "\n",
    "# \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "# \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/65.0.3325.181 Safari/537.36\"\n",
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "}\n",
    "\n",
    "def tagSave(tagName, tagUrl):\n",
    "    # Excel表格初始化\n",
    "    filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', '%s.xlsx'%tagName)\n",
    "    # wb = openpyxl.load_workbook(filePath)\n",
    "    wb = openpyxl.Workbook()\n",
    "    sheet = wb.get_active_sheet()\n",
    "    sheet.title = tagName\n",
    "    fields = ['Label', 'Title', 'Author', 'Rating', 'Comments', 'Link']\n",
    "    for field, alpha in zip(fields, 'ABCDEF'):\n",
    "        sheet[alpha+'1'] = field    \n",
    "    # 读标签内的book信息\n",
    "    total = 0\n",
    "    while True:\n",
    "        tagRes = requests.get(tagUrl, headers=headers)\n",
    "        tagRes.raise_for_status()\n",
    "        tagSoup = bs4.BeautifulSoup(tagRes.text, 'lxml')\n",
    "        bookNums = len(tagSoup.select('h2 a'))\n",
    "        if bookNums == 0:\n",
    "            break\n",
    "        for i in range(bookNums):\n",
    "            sheet['A'+str(i+2+total)] = tagName\n",
    "            sheet['B'+str(i+2+total)] = tagSoup.select('h2 a')[i].get('title')\n",
    "            sheet['C'+str(i+2+total)] = re.compile(r'\\s+').sub('', tagSoup.select('div.pub')[i].text).split('/')[0]\n",
    "            sheet['F'+str(i+2+total)] = tagSoup.select('h2 a')[i].get('href')\n",
    "            if '少于10人评价' in tagSoup.select('.clearfix .pl')[i].text or '无人评价' in tagSoup.select('.clearfix .pl')[i].text:\n",
    "                sheet['D'+str(i+2+total)] = ''\n",
    "                sheet['E'+str(i+2+total)] = 0\n",
    "            else:\n",
    "                sheet['D'+str(i+2+total)] = tagSoup.select('.info .clearfix')[i].select('.rating_nums')[0].text\n",
    "                sheet['E'+str(i+2+total)] = int(re.compile(r'\\d+').search(tagSoup.select('.clearfix .pl')[i].text).group(0))\n",
    "        total = total + bookNums\n",
    "        if tagSoup.select('.next link') == []:\n",
    "            break\n",
    "        else:\n",
    "            tagUrl = 'https://book.douban.com' + tagSoup.select('.next link')[0].get('href')\n",
    "    # 存Excel表格 \n",
    "    wb.save(filePath)\n",
    "    wb.close()\n",
    "\n",
    "# 读取标签名和url\n",
    "url = 'https://book.douban.com/tag'\n",
    "res = requests.get(url)\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "tags = soup.select('.tagCol>tbody>tr>td>a')\n",
    "tagNames = []\n",
    "tagUrls = []\n",
    "for i in range(len(tags)):\n",
    "    tagNames.append(tags[i].text)    \n",
    "    tagUrls.append('https://book.douban.com' + tags[i].get('href'))\n",
    "\n",
    "# 运行函数\n",
    "saveThreads = []\n",
    "for i in range(5):\n",
    "    saveThread = threading.Thread(target=tagSave, args=[tagNames[i], tagUrls[i]])\n",
    "    saveThreads.append(saveThread)\n",
    "    saveThread.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 豆瓣book原文摘录\n",
    "import requests, bs4, os, lxml, re\n",
    "\n",
    "def blockQuotes(fileID, fileName):\n",
    "    url = 'https://book.douban.com/subject/%s/blockquotes'%fileID\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "    }\n",
    "    while True:\n",
    "        res = requests.get(url, headers=headers)\n",
    "        soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "        originalTexts = soup.select('.blockquote-list>ul>li>figure')\n",
    "        filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'douban', '%s.txt'%fileName)\n",
    "        if os.path.exists(filePath):\n",
    "            with open(filePath, 'a') as file:\n",
    "                for text in originalTexts:\n",
    "                    file.write(re.compile(r'\\s+').sub('', text.text) + '\\n' + '\\n')\n",
    "        else:\n",
    "            with open(filePath, 'w') as file:\n",
    "                for text in originalTexts:\n",
    "                    file.write(re.compile(r'\\s+').sub('', text.text) + '\\n' + '\\n')\n",
    "        if soup.select('.next link') != []:\n",
    "            url = 'https://book.douban.com/subject/%s/blockquotes'%fileID + soup.select('.next link')[0].get('href')\n",
    "        else:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated function get_sheet_by_name (Use wb[sheetname]).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# 改成多线程，参数则不同\n",
    "import requests, bs4, lxml, os, openpyxl, re\n",
    "import threading\n",
    "\n",
    "def blockQuotes(startID, EndID):\n",
    "    filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'doubanBooks.xlsx')\n",
    "    wb = openpyxl.load_workbook(filePath)\n",
    "    sheet = wb.get_sheet_by_name('total')\n",
    "    for i in range(startID, EndID):\n",
    "        url = sheet['F'+str(i+1)].value + 'blockquotes'\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "        }\n",
    "        proxies = {           \n",
    "            \"http\":\"59.57.149.237:9999\",\n",
    "            \"http\":\"114.239.151.8:808\",\n",
    "            \"http\":\"182.34.32.162:9999\",\n",
    "            \"http\":\"114.239.254.46:9999\",\n",
    "            \"http\":\"113.194.31.30:9999\"\n",
    "        }\n",
    "        while True:\n",
    "            res = requests.get(url, headers=headers, proxies=proxies)\n",
    "            soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "            originalTexts = soup.select('.blockquote-list>ul>li>figure')\n",
    "            fileName = str(i) +  '-' + sheet['A'+str(i+1)].value\n",
    "            filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'douban', '%s.txt'%fileName)\n",
    "            if os.path.exists(filePath):\n",
    "                with open(filePath, 'a') as file:\n",
    "                    for text in originalTexts:\n",
    "                        file.write(re.compile(r'\\s+').sub('', text.text) + '\\n' + '\\n')\n",
    "            else:\n",
    "                with open(filePath, 'w') as file:\n",
    "                    for text in originalTexts:\n",
    "                        file.write(re.compile(r'\\s+').sub('', text.text) + '\\n' + '\\n')\n",
    "            if soup.select('.next link') != []:\n",
    "                url = sheet['F'+str(i+1)].value + 'blockquotes' + soup.select('.next link')[0].get('href')\n",
    "            else:\n",
    "                break\n",
    "    wb.close()\n",
    "# 放弃多线程，因为要对同一个文件进行读取。\n",
    "blockQuotes(411, 421)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.7 赵雅芝贴吧内容读取 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.1 存入字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '【典雅一生】《新白娘子传奇》截图帖（不定时更新）', 'replies': '2474', 'link': 'http://tieba.baidu.com/p/4983620050'}\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml\n",
    "\n",
    "info = []\n",
    "for i in range(0, 20000, 50):\n",
    "    url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E9%9B%85%E8%8A%9D&ie=utf-8&pn=' + str(i)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    yazhiSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    titles = yazhiSoup.select('a[class=\"j_th_tit\"]')\n",
    "    replies = yazhiSoup.select('span[class=\"threadlist_rep_num center_text\"]')\n",
    "    for title, reply in zip(titles, replies):\n",
    "        data = {\n",
    "            \"title\":title.get_text(),\n",
    "            \"replies\":reply.get_text(),\n",
    "            \"link\":'http://tieba.baidu.com' + title['href']\n",
    "        }\n",
    "        info.append(data)\n",
    "\n",
    "for i in info:\n",
    "    if '新白娘子传奇' in i['title'] and int(i['replies'])>2000:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.2 存入text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests, bs4, lxml, os\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'yazhitieba', 'yazhi.txt')\n",
    "with open(path, 'w+') as file_txt:\n",
    "    file_txt.write('-------------------Title-----------------replies---------------link--------------\\n')\n",
    "    for i in range(0, 1000, 50):\n",
    "        url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E9%9B%85%E8%8A%9D&ie=utf-8&pn=' + str(i)\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "        yazhiSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "        titles = yazhiSoup.select('a[class=\"j_th_tit\"]')\n",
    "        replies = yazhiSoup.select('span[class=\"threadlist_rep_num center_text\"]')\n",
    "        for title, reply in zip(titles, replies):\n",
    "            data = {\n",
    "                \"title\":title.get_text(),\n",
    "                \"replies\":reply.get_text(),\n",
    "                \"link\":'http://tieba.baidu.com' + title['href']\n",
    "            }        \n",
    "            file_txt.write(data['title'] + '  ' + data['replies'] + '  ' + data['link'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.3 写入Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml, openpyxl, os\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'Tieba'\n",
    "sheet['A1'] = 'Title'\n",
    "sheet['B1'] = 'Replies'\n",
    "sheet['C1'] = 'Link'\n",
    "total = 0\n",
    "\n",
    "for i in range(0, 20000, 50):\n",
    "    url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E9%9B%85%E8%8A%9D&ie=utf-8&pn=' + str(i)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    yazhiSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    titles = yazhiSoup.select('a[class=\"j_th_tit\"]')\n",
    "    replies = yazhiSoup.select('span[class=\"threadlist_rep_num center_text\"]')\n",
    "    numbers = range(2+total, 52+total)\n",
    "    for title, reply, number in zip(titles, replies, numbers):\n",
    "        sheet['A'+str(number)] = title.get_text()\n",
    "        sheet['B'+str(number)] = reply.get_text()\n",
    "        sheet['C'+str(number)] = 'http://tieba.baidu.com' + title['href']\n",
    "    total = total + len(titles)\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'yazhitieba', 'yazhi.xlsx')\n",
    "wb.save(path)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `selenium`模块\n",
    "\n",
    "折腾过程见教程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 尝试\n",
    "\n",
    "**如果browser赋值时打开的窗口关掉了，让browser.get(url)就会出错。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <img> element with that class name!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.remote.webelement.WebElement"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "url = 'http://inventwithpython.com'\n",
    "browser.get(url)\n",
    "\n",
    "try:\n",
    "    elem = browser.find_element_by_class_name('card-img-top')\n",
    "    print('Found <%s> element with that class name!' %(elem.tag_name))\n",
    "except:\n",
    "    print('Was not able to find an element with that name.')\n",
    "\n",
    "linkElem = browser.find_element_by_link_text('Read Online for Free')\n",
    "type(linkElem)\n",
    "\n",
    "linkElem.click()\n",
    "\n",
    "links = browser.find_elements_by_partial_link_text('free review copy')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真的把这个链接打开了。终于回忆起来**selenium的特性**是到哪里说哪里的话，点击进入哪个页面就只看哪个页面的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "url = 'http://gmail.com'\n",
    "browser.get(url)\n",
    "\n",
    "# 找出input元素：\n",
    "classElem = browser.find_element_by_class_name('whsOnd')\n",
    "idElem = browser.find_element_by_id('identifierId')\n",
    "nameElem = browser.find_element_by_name('identifier')\n",
    "tagElem = browser.find_element_by_tag_name('input')\n",
    "classElem == idElem\n",
    "idElem == nameElem\n",
    "nameElem == tagElem\n",
    "\n",
    "tagsElem_div = browser.find_elements_by_tag_name('div')\n",
    "tagsElem_a = browser.find_elements_by_tag_name('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 登录gmail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 填写email帐号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import getpass\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "url = 'http://gmail.com'\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "email_input = browser.find_element_by_tag_name('input')\n",
    "email_input.clear()\n",
    "email_input.send_keys(getpass.getpass())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图毕其功于一役，将以上两个过程合在一起，失败。错误提示是：**元素不可见**。原因是无法打开gmail页面，也就找不到所谓元素了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 点击下一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "classElem_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 填写密码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "tagsElem_input = browser.find_elements_by_tag_name('input')\n",
    "password = tagsElem_input[2]\n",
    "password.send_keys(getpass.getpass())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 点击下一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "classElem_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是很久不登录需要选择风格的画面。一般不会遇到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameElem_btn = browser.find_element_by_name('welcome_dialog_next')\n",
    "nameElem_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeElem_btn = browser.find_element_by_name('ok')\n",
    "typeElem_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逐步登录成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "————————————合并成一个完整程序————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "from selenium import webdriver\n",
    "import getpass\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "\n",
    "def gmailSignin(email, password):\n",
    "    #打开登录界面\n",
    "    url = 'http://gmail.com'\n",
    "    browser.get(url)\n",
    "\n",
    "    # 填写email帐号\n",
    "    tagsElem_input = browser.find_elements_by_tag_name('input')\n",
    "    emailElem = tagsElem_input[0]\n",
    "    emailElem.clear()\n",
    "    emailElem.send_keys(email)\n",
    "\n",
    "    # 点击下一步\n",
    "    classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "    classElem_btn.click()\n",
    "\n",
    "    # 填写密码\n",
    "    Elem_input = browser.find_element_by_name('password')\n",
    "    print(Elem_input)\n",
    "    Elem_input.send_keys(password)\n",
    "\n",
    "    # 点击下一步\n",
    "    classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "    classElem_btn.click()\n",
    "    \n",
    "email = getpass.getpass('Input your email account: ')\n",
    "password = getpass.getpass('Input your password: ') #不显示输入值\n",
    "gmailSignin(email, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 获取动态网页数据\n",
    "\n",
    "两个工具：  \n",
    "\n",
    "- [在线JSON校验格式化工具（Be JSON）](https://www.bejson.com/)\n",
    "- [JSON在线解析及格式化验证 - JSON.cn](https://www.json.cn/)\n",
    "\n",
    "一堆教程：\n",
    "\n",
    "> 永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。\n",
    "\n",
    "    关于拉勾网的两篇教程，要连起来看：\n",
    "\n",
    "- [Web crawler with Python - 04.另一种抓取方式 - 知乎](https://zhuanlan.zhihu.com/p/20430122)\n",
    "- [Python搭建代理池爬取拉勾网招聘信息 - 掘金](https://juejin.im/post/5d5e92916fb9a06ac93cd5f5)\n",
    "    \n",
    "    第三篇略复杂：  \n",
    "    \n",
    "- [Python爬取拉钩招聘网，让你清楚了解Python行业 - 掘金](https://juejin.im/post/5dc3ce0a6fb9a04aba52b643)\n",
    "\n",
    "    第四篇：\n",
    "    \n",
    "- [拉勾网反爬虫解决方法 | lijun Blog](https://darkless.cn/2019/05/25/lagou-crawl-solution/)\n",
    "\n",
    "    关于今日头条的教程：  \n",
    "\n",
    "- [Python 网络爬虫：解析JSON, 获取JS动态内容—爬取今日头条, 抓取json内容 - Just Code](http://justcode.ikeepstudying.com/2018/12/python-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%9A%E8%A7%A3%E6%9E%90json-%E8%8E%B7%E5%8F%96js%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9-%E7%88%AC%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1/)  \n",
    "\n",
    "    关于淘宝的教程：  \n",
    "\n",
    "- [用python抓取淘宝评论 - 云+社区 - 腾讯云](https://cloud.tencent.com/developer/article/1059747)  \n",
    "- [Python 从零开始爬虫(五)——初遇json&爬取某宝商品信息 - Python 从零开始爬虫 - SegmentFault 思否](https://segmentfault.com/a/1190000014688216)  \n",
    "\n",
    "\n",
    "### 5.1 拉勾网  \n",
    "\n",
    "#### 5.1.1 `requests`和`bs`的常规做法：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4, lxml, os\n",
    "\n",
    "url = 'https://www.lagou.com/zhaopin/Python/?labelWords=label'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "soup.select('.list_item_top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果理所当然为空。因为数据是js动态发送的。  \n",
    "\n",
    "#### 5.1.2 找json数据  \n",
    "\n",
    "在F12（option+command+I）下，以前都是看elements，现在改看network，观察其中的XHR部分，在页面刷新或者点击页面上的一些切换按钮（总之就是让页面产生变化）时，看这一部分新增加的有哪些。取网址读取其中的json数据进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': False,\n",
       " 'msg': '您操作太频繁,请稍后再访问',\n",
       " 'clientIp': '221.225.172.92',\n",
       " 'state': 2408}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json, lxml, os\n",
    "\n",
    "# 点击了职位选项后真的看到了positionAjax，通过右击copy link address或header里的url或双击打开取url，都能获得下面这个url。\n",
    "url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&needAddtionalResult=false'\n",
    "res = requests.post(url)\n",
    "res.raise_for_status()\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3 分析URL参数\n",
    "\n",
    "```\n",
    "url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&needAddtionalResult=false'\n",
    " \n",
    "px = default\n",
    "needAddtionalResult = false\n",
    "```\n",
    "\n",
    "#### 5.1.4  增加cookie\n",
    "\n",
    "上面的结果，和[Python搭建代理池爬取拉勾网招聘信息 - 掘金](https://juejin.im/post/5d5e92916fb9a06ac93cd5f5)的描述一样，没有获得真正的data数据。  \n",
    "\n",
    "从headers中可提取出如下数据：  \n",
    "\n",
    "```\n",
    "Request Method: POST \n",
    "Accept: application/json, text/javascript, */*; q=0.01\n",
    "Cookie: 略\n",
    "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\n",
    "Referer: https://www.lagou.com/jobs/list_Python/p-city_0?px=default\n",
    "Content-Type: application/x-www-form-urlencoded; charset=UTF-8\n",
    "From Data  \n",
    "\n",
    "    first: true\n",
    "    pn: 1\n",
    "    kd: Python\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'content'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-327c19a0adab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'content'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'positionResult'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'result'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# 存excel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'content'"
     ]
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"www.lagou.com\",\n",
    "    \"Referer\": 'https://www.lagou.com/jobs/list_Python/p-city_0?px=default',\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\",\n",
    "    \"Cookie\": \"JSESSIONID=ABAAABAAAIAACBIF4CE656D7045142F03438F07953A2EC5; user_trace_token=20191112134949-99c0fe75-1522-4700-9299-277ae02d8684; WEBTJ-ID=20191112134949-16e5e29370fcd-07a51ff750fd41-1c3c6a5a-1049088-16e5e293710479; _ga=GA1.2.1632364892.1573537790; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1573182236; LGUID=20191112134950-3ddaf708-0510-11ea-a62d-5254005c3644; _gid=GA1.2.1406510492.1573537790; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2216e5e2939bb3cd-016f07345976e3-1c3c6a5a-1049088-16e5e2939bcd7%22%2C%22%24device_id%22%3A%2216e5e2939bb3cd-016f07345976e3-1c3c6a5a-1049088-16e5e2939bcd7%22%7D; index_location_city=%E5%85%A8%E5%9B%BD; _gat=1; LGSID=20191113092033-ca0183c5-05b3-11ea-a4f3-525400f775ce; PRE_UTM=; PRE_HOST=; PRE_SITE=https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_Python%2Fp-city_0%3Fpx%3Ddefault%26gx%3D%25E5%2585%25A8%25E8%2581%258C%26gj%3D%26isSchoolJob%3D1; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_Python%2Fp-city_0%3Fpx%3Ddefault%26gx%3D%26isSchoolJob%3D1; TG-TRACK-CODE=index_navigation; X_HTTP_TOKEN=004a2ca20ecbebc496380637513c48bf18724f2cd0; LGRID=20191113092609-929473ea-05b4-11ea-a62d-5254005c3644; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1573608370; SEARCH_ID=6303c489a71c4875a6cc7d1d88198d63\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"first\": True,\n",
    "    \"pn\": 1,\n",
    "    \"kd\": \"python\"\n",
    "}\n",
    "\n",
    "url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&needAddtionalResult=false'\n",
    "res = requests.post(url, headers=headers, data=data)\n",
    "res.raise_for_status()\n",
    "res.encoding = 'utf-8'\n",
    "result = res.json()['content']['positionResult']['result']\n",
    "\n",
    "# 存excel\n",
    "import openpyxl, os\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'PythonPosition'\n",
    "sheet['A1'], sheet['B1'], sheet['C1'], sheet['D1'] = 'positionName','companyFullName','skillLables','createTime'\n",
    "sheet['E1'], sheet['F1'], sheet['G1'] = 'salary','workYear','jobNature'\n",
    "sheet['H1'], sheet['I1'], sheet['J1'] = 'address', 'latitude', 'longitude'\n",
    "\n",
    "for i, p in zip(range(2, len(result)+2), result):\n",
    "    sheet['A' + str(i)] = result[i-2]['positionName']\n",
    "    sheet['B' + str(i)] = result[i-2]['companyFullName']\n",
    "    sheet['C' + str(i)] = ''.join(result[i-2]['skillLables'])\n",
    "    sheet['D' + str(i)] = result[i-2]['createTime']\n",
    "    sheet['E' + str(i)] = result[i-2]['salary']\n",
    "    sheet['F' + str(i)] = result[i-2]['workYear']\n",
    "    sheet['G' + str(i)] = result[i-2]['jobNature']\n",
    "    sheet['H' + str(i)] = ','.join([result[i-2]['city'], result[i-2]['district']])\n",
    "    sheet['I' + str(i)] = result[i-2]['latitude']\n",
    "    sheet['J' + str(i)] = result[i-2]['longitude']\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'lagou', 'lagou.xlsx')\n",
    "wb.save(path)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图取得第二页数据，但是和前几天一样结果，且换IP也没有用。第一页数据好像梦中出现一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'status': False,\n",
       " 'msg': '您操作太频繁,请稍后再访问',\n",
       " 'clientIp': '49.78.9.32',\n",
       " 'state': 2402}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"www.lagou.com\",\n",
    "    \"Origin\": \"https://www.lagou.com\",\n",
    "    \"Referer\": \"https://www.lagou.com/jobs/list_Python/p-city_0?px=default\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\"\n",
    "}\n",
    "\n",
    "proxies = {\n",
    "    \"http\":\"59.57.149.237:9999\",\n",
    "    \"http\":\"114.239.151.8:808\",\n",
    "    \"http\":\"182.34.32.162:9999\",\n",
    "    \"http\":\"114.239.254.46:9999\",\n",
    "    \"http\":\"113.194.31.30:9999\"\n",
    "}\n",
    "\n",
    "session = requests.session()\n",
    "session.get(url, headers=headers, proxies=proxies)\n",
    "\n",
    "data = {\n",
    "    \"first\": \"true\",\n",
    "    \"pn\": 1,\n",
    "    \"kd\": \"Python\"\n",
    "}\n",
    "\n",
    "rep = session.post(url, headers=headers, proxies=proxies, data=data)\n",
    "rep.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': False,\n",
       " 'msg': '您操作太频繁,请稍后再访问',\n",
       " 'clientIp': '180.117.236.114',\n",
       " 'state': 2402}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"www.lagou.com\",\n",
    "    \"Referer\": 'https://www.lagou.com/jobs/list_Python/p-city_0?px=default',\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\",\n",
    "    \"Cookie\": \"JSESSIONID=ABAAABAAAIAACBIF4CE656D7045142F03438F07953A2EC5; user_trace_token=20191112134949-99c0fe75-1522-4700-9299-277ae02d8684; WEBTJ-ID=20191112134949-16e5e29370fcd-07a51ff750fd41-1c3c6a5a-1049088-16e5e293710479; _ga=GA1.2.1632364892.1573537790; Hm_lvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1573182236; LGUID=20191112134950-3ddaf708-0510-11ea-a62d-5254005c3644; _gid=GA1.2.1406510492.1573537790; sensorsdata2015jssdkcross=%7B%22distinct_id%22%3A%2216e5e2939bb3cd-016f07345976e3-1c3c6a5a-1049088-16e5e2939bcd7%22%2C%22%24device_id%22%3A%2216e5e2939bb3cd-016f07345976e3-1c3c6a5a-1049088-16e5e2939bcd7%22%7D; index_location_city=%E5%85%A8%E5%9B%BD; _gat=1; LGSID=20191113092033-ca0183c5-05b3-11ea-a4f3-525400f775ce; PRE_UTM=; PRE_HOST=; PRE_SITE=https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_Python%2Fp-city_0%3Fpx%3Ddefault%26gx%3D%25E5%2585%25A8%25E8%2581%258C%26gj%3D%26isSchoolJob%3D1; PRE_LAND=https%3A%2F%2Fwww.lagou.com%2Fjobs%2Flist_Python%2Fp-city_0%3Fpx%3Ddefault%26gx%3D%26isSchoolJob%3D1; TG-TRACK-CODE=index_navigation; X_HTTP_TOKEN=004a2ca20ecbebc496380637513c48bf18724f2cd0; LGRID=20191113092609-929473ea-05b4-11ea-a62d-5254005c3644; Hm_lpvt_4233e74dff0ae5bd0a3d81c6ccf756e6=1573608370; SEARCH_ID=6303c489a71c4875a6cc7d1d88198d63\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"first\": False,\n",
    "    \"pn\": 2,\n",
    "    \"kd\": \"python\"\n",
    "}\n",
    "\n",
    "url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&needAddtionalResult=false'\n",
    "res = requests.post(url, headers=headers, data=data)\n",
    "res.raise_for_status()\n",
    "res.encoding = 'utf-8'\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 今日头条\n",
    "\n",
    "#### 5.2.1 初始尝试  \n",
    "\n",
    "这次仅仅找到了右侧“24小时热文”的链接"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, bs4, lxml\n",
    "\n",
    "url = 'https://www.toutiao.com/api/pc/realtime_news/' # 右击copy link address\n",
    "res = requests.get(url)\n",
    "data = json.loads(res.text)\n",
    "\n",
    "for i in range(len(data['data'])) :\n",
    "    newsUrl = 'https://www.toutiao.com' + data['data'][i]['open_url']\n",
    "    # print(newsUrl)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2 找到了正content列表链接\n",
    "\n",
    "没有加headers和data时，数据取不出来。加了后，连续运行几次都可以取出。这点似乎比拉勾网不同。  \n",
    "但是批量取页还是不成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:54: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n"
     ]
    }
   ],
   "source": [
    "import requests, json, openpyxl, os\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"text/javascript, text/html, application/xml, text/xml, */*\",\n",
    "    \"Referer\": 'https://www.toutiao.com/',\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\",\n",
    "    \"Cookie\": \"tt_webid=6756795573083686408; s_v_web_id=c3d479b6049b458d283d183b5d273435; WEATHER_CITY=%E5%8C%97%E4%BA%AC; tt_webid=6756795573083686408; csrftoken=99c459a02dd8aa3bd451f4eda8c97e65; UM_distinctid=16e49816ff08a-0be5c23d474e65-1c3c6a5a-100200-16e49816ff14cf; CNZZDATA1259612802=1054615425-1573187717-https%253A%252F%252Fwww.toutiao.com%252F%7C1573187717; _ga=GA1.2.1403944884.1573191251; __tasessionId=gxxa0jjg41573695423978\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"min_behot_time\": 0,\n",
    "    \"category\": \"__all__\",\n",
    "    \"utm_source\": \"toutiao\",\n",
    "    \"widen\": 1,\n",
    "    \"tadrequire\": True,\n",
    "    \"as\": \"A1D56D4C1C8B28D\",\n",
    "    \"cp\": \"5DCC2B02C87DBE1\",\n",
    "    \"_signature\": \"WFkevAAgEB3UTJNZiPzNflhZHqAAAWW\"\n",
    "}\n",
    "\n",
    "url = 'https://www.toutiao.com/api/pc/feed/?min_behot_time=0&category=__all__&utm_source=toutiao&widen=1&tadrequire=true&as=A1D56D4C1C8B28D&cp=5DCC2B02C87DBE1&_signature=WFkevAAgEB3UTJNZiPzNflhZHqAAAWW'\n",
    "res = requests.get(url, headers=headers, data=data)\n",
    "data = json.loads(res.text)\n",
    "result1 = data['data']\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"text/javascript, text/html, application/xml, text/xml, */*\",\n",
    "    \"Referer\": 'https://www.toutiao.com/',\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\",\n",
    "    \"Cookie\": \"tt_webid=6756795573083686408; s_v_web_id=c3d479b6049b458d283d183b5d273435; WEATHER_CITY=%E5%8C%97%E4%BA%AC; tt_webid=6756795573083686408; csrftoken=99c459a02dd8aa3bd451f4eda8c97e65; UM_distinctid=16e49816ff08a-0be5c23d474e65-1c3c6a5a-100200-16e49816ff14cf; CNZZDATA1259612802=1054615425-1573187717-https%253A%252F%252Fwww.toutiao.com%252F%7C1573187717; _ga=GA1.2.1403944884.1573191251; __tasessionId=gxxa0jjg41573695423978\"\n",
    "}\n",
    "\n",
    "data = {\n",
    "    \"max_behot_time\": 1573699086,\n",
    "    \"category\": \"__all__\",\n",
    "    \"utm_source\": \"toutiao\",\n",
    "    \"widen\": 1,\n",
    "    \"tadrequire\": True,\n",
    "    \"as\": \"A1253D4CFC8C053\",\n",
    "    \"cp\": \"5DCC5C2055634E1\",\n",
    "    \"_signature\": \"NJMTMAAgEB64hp7VRhDaLTSTEyAAGlm\"\n",
    "}\n",
    "url = 'https://www.toutiao.com/api/pc/feed/?max_behot_time=1573699086&category=__all__&utm_source=toutiao&widen=1&tadrequire=true&as=A1253D4CFC8C053&cp=5DCC5C2055634E1&_signature=NJMTMAAgEB64hp7VRhDaLTSTEyAAGlm'\n",
    "res = requests.get(url, headers=headers, data=data)\n",
    "data = json.loads(res.text)\n",
    "result2 = data['data']\n",
    "\n",
    "# 存excel\n",
    "import openpyxl, os, time\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'toutiaoNews'\n",
    "sheet['A1'], sheet['B1'], sheet['C1']  = 'title','abstract','comments_count'\n",
    "sheet['D1'], sheet['E1'] = 'behot_time','sourceMedia'\n",
    "total = 0\n",
    "for i, p in zip(range(2, len(result1)+2), result1):\n",
    "    sheet['A' + str(i)] = result1[i-2]['title']\n",
    "    sheet['A' + str(i)].hyperlink = 'https://www.toutiao.com' + result1[i-2]['source_url']\n",
    "    sheet['B' + str(i)] = result1[i-2]['abstract']\n",
    "    sheet['C' + str(i)] = result1[i-2].get('comments_count', 0) # 偶尔会没有评论数，会有keyerror错误\n",
    "    sheet['D' + str(i)] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(result1[i-2]['behot_time']))\n",
    "    sheet['E' + str(i)] = result1[i-2]['source']\n",
    "    sheet['E' + str(i)].hyperlink = 'https://www.toutiao.com' + result1[i-2].get('media_url', 'null')\n",
    "total = total + len(result1)\n",
    "for i, p in zip(range(2, len(result2)+2), result2):\n",
    "    sheet['A' + str(i+total)] = result2[i-2]['title']\n",
    "    sheet['A' + str(i+total)].hyperlink = 'https://www.toutiao.com' + result2[i-2]['source_url']\n",
    "    sheet['B' + str(i+total)] = result2[i-2]['abstract']\n",
    "    sheet['C' + str(i+total)] = result2[i-2].get('comments_count', 0) # 偶尔会没有评论数，会有keyerror错误\n",
    "    sheet['D' + str(i+total)] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(result2[i-2]['behot_time']))\n",
    "    sheet['E' + str(i+total)] = result2[i-2]['source']\n",
    "    sheet['E' + str(i+total)].hyperlink = 'https://www.toutiao.com' + result2[i-2].get('media_url', 'null')\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'toutiao.xlsx')\n",
    "wb.save(path)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 淘宝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python format 格式化函数 | 菜鸟教程](https://www.runoob.com/python/att-string-format.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, lxml, json\n",
    "\n",
    "url = 'https://item.taobao.com/item.htm?spm=a219r.lm874.14.13.292512d5Thbdic&id=599063101905&ns=1&abbucket=9'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "url = 'https://s.taobao.com/search?\\\n",
    "       q={name}&\\\n",
    "       imgfile=&\\\n",
    "       js=1&\\\n",
    "       stats_click=search_radio_all%3A1&\\\n",
    "       initiative_id=staobaoz_{date}&\\\n",
    "       ie=utf8&\\\n",
    "       sort={sort}&\\\n",
    "       s={num}'.format(name='zara', date='20191109', sort='price-asc', num=88)\n",
    "# sort：默认排序（按综合排），default；\n",
    "# sort：按销量排，sale-desc；\n",
    "# sort：按信用拍，credit-desc；\n",
    "# sort：按价格从低到高，price-asc；\n",
    "# sort：按价格从高到低，price-desc；\n",
    "# sort：总价从低到高，total-asc；\n",
    "# sort：总价从高到低，total-desc；\n",
    "\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 作业——测绘资质\n",
    "\n",
    "第一次尝试时，以为需要进入iframe读取，那会儿不知道js传送实时数据，就只看到内容在一个iframe里。但是啥也没成功。\n",
    "不过使用的暂停和后来的不一样，是：`browser.implicitly_wait(20) #让浏览器等待一下，最长等10秒。看看数据有没有加载出来。`   \n",
    "事实证明：这句不如`time.sleep(2)`好用。\n",
    "\n",
    "附上上次记录的教程：  \n",
    "[Python selenium —— 一定要会用selenium的等待，三种等待方式解读 - 灰蓝 - CSDN博客](https://blog.csdn.net/huilan_same/article/details/52544521)\n",
    "\n",
    "这里有个嵌套frame的应对办法：[python - Selenium Webdriver give NoSuchFrameException - Stack Overflow](https://stackoverflow.com/questions/28778142/selenium-webdriver-give-nosuchframeexception) \n",
    "\n",
    "保留第一次辛辛苦苦写的省份字典，其他删了：  \n",
    "```\n",
    "provinces = {\n",
    "    '北 京':'BJ', '上 海':'SH', '天 津':'TJ', '重 庆':'CQ', '河 北':'HEB', '河 南':'HEN', '山 东':'SD', '山 西':'SX', '湖 北':'HB',\\\n",
    "    '湖 南':'HN', '辽 宁':'LN', '吉 林':'JL', '黑龙江':'HL', '江 苏':'JS', '浙 江':'ZJ', '福 建':'FJ', '安 徽':'AH', '广 东':'GD', \\\n",
    "    '广 西':'GX', '江 西':'JX', '四 川':'SC', '贵 州':'GZ', '海 南':'HI', '云 南':'YN', '陕 西':'SN', '甘 肃':'GS', '青 海':'QH', \\\n",
    "    '宁 夏':'NX', '新 疆':'XJ', '西 藏':'XZ', '内蒙古':'NM'\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "重新做。刚开始以为js发送的是json数据，但其实是table。于是摒除json数据想法，老老实实用selenium点击浏览器。  \n",
    "这次多亏了学会从networks中找js\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, os, openpyxl, time\n",
    "from selenium import webdriver\n",
    "\n",
    "def downloadCehui(prov, url, pageNum):\n",
    "    # 打开页面\n",
    "    path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "    browser = webdriver.Chrome(path)\n",
    "    browser.get(url)\n",
    "    browser.find_element_by_id('BtnSearch').click()\n",
    "    browser.maximize_window()  #浏览器最大化\n",
    "    # Excel表格初始化\n",
    "    filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'cehuizizhi.xlsx')\n",
    "    wb = openpyxl.load_workbook(filePath) # 追加已有表格。如果新建表格用openpyxl.Workbook()\n",
    "    sheet = wb.create_sheet()\n",
    "    sheet.title = prov\n",
    "    for alpha, i in zip('ABCDE', range(5)):\n",
    "        field = browser.find_elements_by_css_selector('#UpdatePanel5>table>tbody>tr[class=formbg]>td')\n",
    "        sheet[alpha + str(1)] = field[i].text\n",
    "    # 存储页面数据\n",
    "    total = 0\n",
    "    for num in range(pageNum):\n",
    "        try:\n",
    "            number = len(browser.find_elements_by_css_selector('#UpdatePanel5>table>tbody>tr[style=\"line-height: 23px;\"]'))\n",
    "            for i in range(number):\n",
    "                for alpha, j in zip('ABCDE', range(5)):\n",
    "                    trTab = browser.find_elements_by_css_selector('#UpdatePanel5>table>tbody>tr[style=\"line-height: 23px;\"]')\n",
    "                    sheet[alpha + str(i+2+total)] = trTab[i].find_elements_by_css_selector('td')[j].text\n",
    "            total = total + number\n",
    "            nextPage = browser.find_elements_by_css_selector('a>img[src=\"/images/PageNavi/nextn.gif\"]')[0]\n",
    "            nextPage.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            break\n",
    "    # 存储Excel表格\n",
    "    wb.save(filePath)\n",
    "    wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://bjchzz.ch.mnr.gov.cn/Index/QueryList.aspx?AreaId=2'  #北京\n",
    "pageNum = 23\n",
    "prov = 'bj'\n",
    "downloadCehui(prov, url, pageNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://tjchzz.ch.mnr.gov.cn/Index/QueryList.aspx?AreaId=21' #天津\n",
    "pageNum = 14\n",
    "prov = 'tj'\n",
    "downloadCehui(prov, url, pageNum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://hebchzz.ch.mnr.gov.cn/Index/QueryList.aspx?AreaId=28'#河北\n",
    "pageNum = 64\n",
    "prov = 'heb'\n",
    "downloadCehui(prov, url, pageNum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 知乎  \n",
    "\n",
    "想着找专栏看看。按照上面的经验写了后，解析json时出现了一个错误：  \n",
    "\n",
    "```\n",
    "JSONDecodeError: Expecting value: line 1 column 1 (char 0)\n",
    "```\n",
    "\n",
    "在这里[python - JSONDecodeError: Expecting value: line 1 column 1 (char 0) - Stack Overflow](https://stackoverflow.com/questions/16573332/jsondecodeerror-expecting-value-line-1-column-1-char-0)找到说：只要headers里去掉\"Accept-Encoding\": \"gzip, deflate, br\"即可。试了果然成功。很多其他答案还以为res的返回代码不正常导致的呢。\n",
    "\n",
    "这个url好，只要改了limit的值（初始值是6），就可以不加任何代码，多下载。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import requests, json, openpyxl, os, time\n",
    "\n",
    "# Excel表格初始化\n",
    "filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'zhihu.xlsx')\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'zhuanlan'\n",
    "fields = ['Title', 'Description', 'Articles_count', 'ID', 'Intro', 'Followers', 'Created', 'Updated', 'URL']\n",
    "for field, alpha in zip(fields, 'ABCDEFGHI'):\n",
    "    sheet[alpha + '1'] = field\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "    \"Origin\": \"https://zhuanlan.zhihu.com\",\n",
    "    \"Referer\": \"https://zhuanlan.zhihu.com/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\",\n",
    "    \"Cookie\":'_zap=4f087203-bde1-469b-95d7-bec4f36c7893; d_c0=\"AIArtCd7iQ-PTqFWBtSg-yAwhjme3UJPnX0=|1559704236\"; z_c0=\"2|1:0|10:1560992880|4:z_c0|92:Mi4xRDAwQUFBQUFBQUFBZ0N1MEozdUpEeVlBQUFCZ0FsVk5jQ3I0WFFBeWl2R1phaUxIbjZqRlc0NW9uQTNQeFJBMzhR|b17c2924dee3cb1868739bca6785dfc4dd7268b5af85498660c6fdc0feba7dc5\"; __utmv=51854390.100--|2=registration_date=20110513=1^3=entry_date=20110513=1; __gads=ID=cca6833400db0db9:T=1561517189:S=ALNI_MZTpiPGOFaaZQsdAb1Jvru-1Zm3mw; __utma=51854390.143703964.1560993917.1562660822.1564392440.6; __utmz=51854390.1564392440.6.6.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/cao-mei-juan/collections; tst=r; _xsrf=j44aK1MNZhzDPcsBpwuu1Dgra5E8LAmh; q_c1=0bfc58ff4bb9453896e281ade35242f0|1573974697000|1560222725000; tgw_l7_route=73af20938a97f63d9b695ad561c4c10c; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1574297705,1574298200,1574303217,1574309266; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1574309635'\n",
    "}\n",
    "\n",
    "pn = 0\n",
    "page = 6\n",
    "total = 0\n",
    "while True:\n",
    "    url = 'https://zhuanlan.zhihu.com/api/recommendations/columns?limit=%d&offset=%d&seed=7'%(page, page*pn)\n",
    "    res = requests.get(url, headers=headers)\n",
    "    isEnd = json.loads(res.text)['paging']['is_end']\n",
    "    if isEnd == 'true':\n",
    "        break\n",
    "    else:\n",
    "        result = json.loads(res.text)['data']\n",
    "        for i in range(len(result)):\n",
    "            sheet['A' + str(i+2+total)] = result[i]['title']\n",
    "            sheet['B' + str(i+2+total)] = result[i]['description']\n",
    "            sheet['C' + str(i+2+total)] = result[i]['articles_count']\n",
    "            sheet['D' + str(i+2+total)] = result[i]['id']\n",
    "            sheet['E' + str(i+2+total)] = result[i]['intro']\n",
    "            sheet['F' + str(i+2+total)] = result[i]['followers']\n",
    "            sheet['G' + str(i+2+total)] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(result[i]['created']))\n",
    "            sheet['H' + str(i+2+total)] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(result[i]['updated']))\n",
    "            sheet['I' + str(i+2+total)] = result[i]['url']\n",
    "        total = total + len(result)\n",
    "        pn = pn + 1\n",
    "        wb.save(filePath)\n",
    "wb.save(filePath)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:6: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "6\n",
      "12\n",
      "6\n",
      "18\n",
      "6\n",
      "24\n",
      "6\n",
      "30\n"
     ]
    }
   ],
   "source": [
    "import requests, json, openpyxl, os, time\n",
    "\n",
    "# Excel表格初始化\n",
    "filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'zhihu.xlsx')\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'zhuanlan'\n",
    "fields = ['Title', 'Description', 'Articles_count', 'ID', 'Intro', 'Followers', 'Created', 'Updated', 'URL']\n",
    "for field, alpha in zip(fields, 'ABCDEFGHI'):\n",
    "    sheet[alpha + '1'] = field\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": \"*/*\",\n",
    "    \"Accept-Language\": \"zh-CN,zh;q=0.9,en;q=0.8\",\n",
    "    \"Origin\": \"https://zhuanlan.zhihu.com\",\n",
    "    \"Referer\": \"https://zhuanlan.zhihu.com/\",\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.108 Safari/537.36\",\n",
    "    \"Cookie\":'_zap=4f087203-bde1-469b-95d7-bec4f36c7893; d_c0=\"AIArtCd7iQ-PTqFWBtSg-yAwhjme3UJPnX0=|1559704236\"; z_c0=\"2|1:0|10:1560992880|4:z_c0|92:Mi4xRDAwQUFBQUFBQUFBZ0N1MEozdUpEeVlBQUFCZ0FsVk5jQ3I0WFFBeWl2R1phaUxIbjZqRlc0NW9uQTNQeFJBMzhR|b17c2924dee3cb1868739bca6785dfc4dd7268b5af85498660c6fdc0feba7dc5\"; __utmv=51854390.100--|2=registration_date=20110513=1^3=entry_date=20110513=1; __gads=ID=cca6833400db0db9:T=1561517189:S=ALNI_MZTpiPGOFaaZQsdAb1Jvru-1Zm3mw; __utma=51854390.143703964.1560993917.1562660822.1564392440.6; __utmz=51854390.1564392440.6.6.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/cao-mei-juan/collections; tst=r; _xsrf=j44aK1MNZhzDPcsBpwuu1Dgra5E8LAmh; q_c1=0bfc58ff4bb9453896e281ade35242f0|1573974697000|1560222725000; tgw_l7_route=73af20938a97f63d9b695ad561c4c10c; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1574297705,1574298200,1574303217,1574309266; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1574309635'\n",
    "}\n",
    "\n",
    "page = 6\n",
    "total = 0\n",
    "for pn in range(5):\n",
    "    url = 'https://zhuanlan.zhihu.com/api/recommendations/columns?limit=%d&offset=%d&seed=7'%(page, page*pn)\n",
    "    res = requests.get(url, headers=headers)\n",
    "    isEnd = json.loads(res.text)['paging']['is_end']\n",
    "    result = json.loads(res.text)['data']\n",
    "    print(len(result))\n",
    "    for i in range(len(result)):\n",
    "        sheet['A' + str(i+2+total)] = result[i]['title']\n",
    "        sheet['B' + str(i+2+total)] = result[i]['description']\n",
    "        sheet['C' + str(i+2+total)] = result[i]['articles_count']\n",
    "        sheet['D' + str(i+2+total)] = result[i]['id']\n",
    "        sheet['E' + str(i+2+total)] = result[i]['intro']\n",
    "        sheet['F' + str(i+2+total)] = result[i]['followers']\n",
    "        sheet['G' + str(i+2+total)] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(result[i]['created']))\n",
    "        sheet['H' + str(i+2+total)] = time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime(result[i]['updated']))\n",
    "        sheet['I' + str(i+2+total)] = result[i]['url']\n",
    "    total = total + len(result)\n",
    "    print(total)\n",
    "wb.save(filePath)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googling...\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "HTTPConnectionPool(host='www.google.com', port=80): Max retries exceeded with url: /search?q=site:zhuanlan.zhihu.com (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x105acd0b8>: Failed to establish a new connection: [Errno 60] Operation timed out'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    158\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 159\u001b[0;31m                 (self._dns_host, self.port), self.timeout, **extra_kw)\n\u001b[0m\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource_address\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    599\u001b[0m                                                   \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m                                                   chunked=chunked)\n\u001b[0m\u001b[1;32m    601\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhttplib_request_kw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;34m\"\"\"Send a complete request to the server.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_request\u001b[0;34m(self, method, url, body, headers, encode_chunked)\u001b[0m\n\u001b[1;32m   1274\u001b[0m             \u001b[0mbody\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_encode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'body'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendheaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mendheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1223\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mCannotSendHeader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1224\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_body\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencode_chunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencode_chunked\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1015\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1016\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1017\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    955\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_open\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 956\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    957\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 168\u001b[0;31m                 self, \"Failed to establish a new connection: %s\" % e)\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.HTTPConnection object at 0x105acd0b8>: Failed to establish a new connection: [Errno 60] Operation timed out",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    637\u001b[0m             retries = retries.increment(method, url, error=e, _pool=self,\n\u001b[0;32m--> 638\u001b[0;31m                                         _stacktrace=sys.exc_info()[2])\n\u001b[0m\u001b[1;32m    639\u001b[0m             \u001b[0mretries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    398\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 399\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    400\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='www.google.com', port=80): Max retries exceeded with url: /search?q=site:zhuanlan.zhihu.com (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x105acd0b8>: Failed to establish a new connection: [Errno 60] Operation timed out'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-bf1ea892b270>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'site:zhuanlan.zhihu.com'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mgoogleit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-bf1ea892b270>\u001b[0m in \u001b[0;36mgoogleit\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Googling...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'User-Agent'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Mozilla/8.0 (compatible; MSIE 8.0; Windows 7)'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPConnectionPool(host='www.google.com', port=80): Max retries exceeded with url: /search?q=site:zhuanlan.zhihu.com (Caused by NewConnectionError('<urllib3.connection.HTTPConnection object at 0x105acd0b8>: Failed to establish a new connection: [Errno 60] Operation timed out'))"
     ]
    }
   ],
   "source": [
    "# _*_coding:utf-8_*_\n",
    "import requests, bs4, re\n",
    "\n",
    "def googleit(query):    \n",
    "    #打开查询结果页面\n",
    "    if '' in query:\n",
    "        query = re.compile(r'\\s+').sub('+', query)\n",
    "    url = 'http://www.google.com/search?q=' + query\n",
    "    print('Googling...')\n",
    "    headers = {'User-Agent':'Mozilla/8.0 (compatible; MSIE 8.0; Windows 7)'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    #选择结果页面\n",
    "    urls = []\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    linkElems = soup.select('.r a')\n",
    "    for i in range(len(linkElems)):\n",
    "        url = 'http://google.com' + linkElems[i].get('href')\n",
    "        urls.append(url)\n",
    "    return urls\n",
    "\n",
    "query = 'site:zhuanlan.zhihu.com'\n",
    "googleit(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  import sys\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method Workbook.close of <openpyxl.workbook.workbook.Workbook object at 0x115bdd7b8>>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获得关注名单\n",
    "import re, bs4, requests, os, openpyxl, json\n",
    "\n",
    "# Excel表格初始化\n",
    "filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'users.xlsx')\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'users'\n",
    "fields = ['name', 'headline', 'articles_count', 'url']\n",
    "for field, alpha in zip(fields, 'ABCD'):\n",
    "    sheet[alpha + '1'] = field\n",
    "\n",
    "url = 'https://www.zhihu.com/api/v4/members/cao-mei-juan/relations/mutuals?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&offset=0&limit=10'\n",
    "headers = {\n",
    "    \"cookie\":'_zap=4f087203-bde1-469b-95d7-bec4f36c7893; d_c0=\"AIArtCd7iQ-PTqFWBtSg-yAwhjme3UJPnX0=|1559704236\"; z_c0=\"2|1:0|10:1560992880|4:z_c0|92:Mi4xRDAwQUFBQUFBQUFBZ0N1MEozdUpEeVlBQUFCZ0FsVk5jQ3I0WFFBeWl2R1phaUxIbjZqRlc0NW9uQTNQeFJBMzhR|b17c2924dee3cb1868739bca6785dfc4dd7268b5af85498660c6fdc0feba7dc5\"; __utmv=51854390.100--|2=registration_date=20110513=1^3=entry_date=20110513=1; __gads=ID=cca6833400db0db9:T=1561517189:S=ALNI_MZTpiPGOFaaZQsdAb1Jvru-1Zm3mw; __utma=51854390.143703964.1560993917.1562660822.1564392440.6; __utmz=51854390.1564392440.6.6.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/cao-mei-juan/collections; _xsrf=j44aK1MNZhzDPcsBpwuu1Dgra5E8LAmh; q_c1=0bfc58ff4bb9453896e281ade35242f0|1573974697000|1560222725000; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1574303217,1574309266,1574317264,1574382663; tst=r; tgw_l7_route=7bacb9af7224ed68945ce419f4dea76d; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1574384964',\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "}\n",
    "res = requests.get(url, headers=headers)\n",
    "result = json.loads(res.text)['data']\n",
    "for i in range(len(result)):\n",
    "    sheet['A' + str(i+2)] = result[i]['name']\n",
    "    sheet['B' + str(i+2)] = result[i]['headline']\n",
    "    sheet['C' + str(i+2)] = result[i]['articles_count']\n",
    "    sheet['D' + str(i+2)] = result[i]['url']\n",
    "wb.save(filePath)\n",
    "wb.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:7: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  import sys\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-361f9597f7e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0msheet\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'A'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "# 我关注的人\n",
    "import re, bs4, requests, os, openpyxl, json\n",
    "\n",
    "# Excel表格初始化\n",
    "filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'users.xlsx')\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'users'\n",
    "fields = ['name', 'headline', 'articles_count', 'url']\n",
    "for field, alpha in zip(fields, 'ABCD'):\n",
    "    sheet[alpha + '1'] = field\n",
    "\n",
    "# 初始url\n",
    "headers = {\n",
    "    \"cookie\":'_zap=4f087203-bde1-469b-95d7-bec4f36c7893; d_c0=\"AIArtCd7iQ-PTqFWBtSg-yAwhjme3UJPnX0=|1559704236\"; z_c0=\"2|1:0|10:1560992880|4:z_c0|92:Mi4xRDAwQUFBQUFBQUFBZ0N1MEozdUpEeVlBQUFCZ0FsVk5jQ3I0WFFBeWl2R1phaUxIbjZqRlc0NW9uQTNQeFJBMzhR|b17c2924dee3cb1868739bca6785dfc4dd7268b5af85498660c6fdc0feba7dc5\"; __utmv=51854390.100--|2=registration_date=20110513=1^3=entry_date=20110513=1; __gads=ID=cca6833400db0db9:T=1561517189:S=ALNI_MZTpiPGOFaaZQsdAb1Jvru-1Zm3mw; __utma=51854390.143703964.1560993917.1562660822.1564392440.6; __utmz=51854390.1564392440.6.6.utmcsr=zhihu.com|utmccn=(referral)|utmcmd=referral|utmcct=/people/cao-mei-juan/collections; _xsrf=j44aK1MNZhzDPcsBpwuu1Dgra5E8LAmh; q_c1=0bfc58ff4bb9453896e281ade35242f0|1573974697000|1560222725000; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1574303217,1574309266,1574317264,1574382663; tst=r; tgw_l7_route=7bacb9af7224ed68945ce419f4dea76d; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1574384964',\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "}\n",
    "url = 'https://www.zhihu.com/api/v4/members/cao-mei-juan/followees?include=data%5B*%5D.answer_count%2Carticles_count%2Cgender%2Cfollower_count%2Cis_followed%2Cis_following%2Cbadge%5B%3F(type%3Dbest_answerer)%5D.topics&offset=0&limit=20'\n",
    "total = 0\n",
    "\n",
    "#进入循环\n",
    "while True:\n",
    "    res = requests.get(url, headers=headers)\n",
    "    result = json.loads(res.text)['data']\n",
    "    for i in range(len(result)):\n",
    "        sheet['A' + str(i+2+total)] = result[i]['name']\n",
    "        sheet['B' + str(i+2+total)] = result[i]['headline']\n",
    "        sheet['C' + str(i+2+total)] = result[i]['articles_count']\n",
    "        sheet['D' + str(i+2+total)] = result[i]['url']\n",
    "    total = total + len(result)\n",
    "    wb.save(filePath)\n",
    "    isEnd = json.loads(res.text)['paging']['is_end']\n",
    "    if isEnd == 'true':\n",
    "        break\n",
    "    else:\n",
    "        url = 'https://www.zhihu.com/api/v4' + (json.loads(res.text)['paging']['next']).split('https://www.zhihu.com')[1]\n",
    "wb.close"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 链家成交数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml, openpyxl, os\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'LianJia'\n",
    "sheet['A1'] = '房源'\n",
    "sheet['B1'] = '成交日期'\n",
    "sheet['C1'] = '总价（万）'\n",
    "sheet['D1'] = '单价（元/平方）'\n",
    "sheet['E1'] = '区域'\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/78.0.3904.97 Safari/537.36\"\n",
    "}\n",
    "pn = 0\n",
    "for i in range(100):\n",
    "    url = 'https://su.lianjia.com/chengjiao/gaoxin1/pg'+str(i+1)\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    lianSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "\n",
    "    fangs = lianSoup.select('ul li div div a[target=\"_blank\"]')\n",
    "    dates = lianSoup.select('div[class=\"dealDate\"]')\n",
    "    totals = lianSoup.select('div[class=\"totalPrice\"] span')\n",
    "    units = lianSoup.select('div[class=\"unitPrice\"] span')\n",
    "\n",
    "    for fang, date, total, unit, number in zip(fangs, dates, totals, units, range(2, 32)):\n",
    "        sheet['A'+str(number+pn)] = fang.get_text()\n",
    "        sheet['B'+str(number+pn)] = date.get_text()\n",
    "        sheet['C'+str(number+pn)] = total.get_text()\n",
    "        sheet['D'+str(number+pn)] = unit.get_text()\n",
    "        sheet['E'+str(number+pn)] = '高新区'\n",
    "    pn = pn + 30\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'scrapy', 'lianjia', 'lianjia.xlsx')\n",
    "wb.save(path)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 知服服查询软著"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4, lxml, os\n",
    "\n",
    "url = 'https://www.zhifufu.com/soft'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "soup.select('.name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 天眼查\n",
    "\n",
    "[Python爬取天眼查网站的方法大全 - 知乎](https://zhuanlan.zhihu.com/p/111913499)\n",
    "哎呀，竟然有个专利叫反爬虫系统及方法~哎呀，专利居然是北京金堤科技有限公司的~哎呀，北京金堤科技有限公司的产品竟然叫天眼查~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[简单爬取天眼查数据 附代码 - 知乎](https://zhuanlan.zhihu.com/p/25273167)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----获取基础信息----\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-e702b7746205>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_content\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdriver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'----获取基础信息----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m     \u001b[0mget_basic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m \u001b[1;34m'----获取高管信息----'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[0mget_gg_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-12-e702b7746205>\u001b[0m in \u001b[0;36mget_basic_info\u001b[1;34m(soup)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_basic_info\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0mcompany\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div.company_info_text > p.ng-binding'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m     \u001b[0mfddbr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.td-legalPersonName-value > p > a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mzczb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.td-regCapital-value > p '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "def driver_open():\n",
    "    dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "    dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "\"Mozilla/5.0 (Windows NT 6.1; WOW64; rv:50.0) Gecko/20100101 Firefox/50.0\"\n",
    "    )\n",
    "    driver = webdriver.PhantomJS(executable_path='C:/ProgramData/Anaconda3/phantomjs-2.1.1-windows/bin/phantomjs.exe', desired_capabilities=dcap)\n",
    "    return driver\n",
    "\n",
    "def get_content(driver,url):\n",
    "    driver.get(url)\n",
    "    #等待5秒，更据动态网页加载耗时自定义\n",
    "    time.sleep(5)\n",
    "    # 获取网页内容\n",
    "    content = driver.page_source.encode('utf-8')\n",
    "    driver.close()\n",
    "    soup = BeautifulSoup(content, 'lxml')\n",
    "    return soup\n",
    "\n",
    "def get_basic_info(soup):\n",
    "    company = soup.select('div.company_info_text > p.ng-binding')[0].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "    fddbr = soup.select('.td-legalPersonName-value > p > a')[0].text\n",
    "    zczb = soup.select('.td-regCapital-value > p ')[0].text\n",
    "    zt = soup.select('.td-regStatus-value > p ')[0].text.replace(\"\\n\",\"\").replace(\" \",\"\")\n",
    "    zcrq = soup.select('.td-regTime-value > p ')[0].text\n",
    "    basics = soup.select('.basic-td > .c8 > .ng-binding ')\n",
    "    hy = basics[0].text\n",
    "    qyzch = basics[1].text\n",
    "    qylx = basics[2].text\n",
    "    zzjgdm = basics[3].text\n",
    "    yyqx = basics[4].text\n",
    "    djjg = basics[5].text\n",
    "    hzrq = basics[6].text\n",
    "    tyshxydm = basics[7].text\n",
    "    zcdz = basics[8].text\n",
    "    jyfw = basics[9].text\n",
    "    print(u'公司名称：'+company)\n",
    "    print(u'法定代表人：'+fddbr)\n",
    "    print(u'注册资本：'+zczb)\n",
    "    print(u'公司状态：'+zt)\n",
    "    print(u'注册日期：'+zcrq)\n",
    "    # print basics\n",
    "    print(u'行业：'+hy)\n",
    "    print(u'工商注册号：'+qyzch)\n",
    "    print(u'企业类型：'+qylx)\n",
    "    print(u'组织机构代码：'+zzjgdm)\n",
    "    print(u'营业期限：'+yyqx)\n",
    "    print(u'登记机构：'+djjg)\n",
    "    print(u'核准日期：'+hzrq)\n",
    "    print(u'统一社会信用代码：'+tyshxydm)\n",
    "    print(u'注册地址：'+zcdz)\n",
    "    print(u'经营范围：'+jyfw)\n",
    "\n",
    "def get_gg_info(soup):\n",
    "    ggpersons = soup.find_all(attrs={\"event-name\": \"company-detail-staff\"})\n",
    "    ggnames = soup.select('table.staff-table > tbody > tr > td.ng-scope > span.ng-binding')\n",
    "    # print(len(gg))\n",
    "    for i in range(len(ggpersons)):\n",
    "        ggperson = ggpersons[i].text\n",
    "        ggname = ggnames[i].text\n",
    "    print(ggperson+\" \"+ggname)\n",
    "\n",
    "def get_gd_info(soup):\n",
    "    tzfs = soup.find_all(attrs={\"event-name\": \"company-detail-investment\"})\n",
    "    for i in range(len(tzfs)):\n",
    "        tzf_split = tzfs[i].text.replace(\"\\n\",\"\").split()\n",
    "        tzf = ' '.join(tzf_split)\n",
    "    print(tzf)\n",
    "\n",
    "def get_tz_info(soup):\n",
    "    btzs = soup.select('a.query_name')\n",
    "    for i in range(len(btzs)):\n",
    "        btz_name = btzs[i].select('span')[0].text\n",
    "    print(btz_name)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    url = \"http://www.tianyancha.com/company/2310290454\"\n",
    "    driver = driver_open()\n",
    "    soup = get_content(driver, url)\n",
    "    print( '----获取基础信息----')\n",
    "    get_basic_info(soup)\n",
    "    print( '----获取高管信息----')\n",
    "    get_gg_info(soup)\n",
    "    print( '----获取股东信息----')\n",
    "    get_gd_info(soup)\n",
    "    print( '----获取对外投资信息----')\n",
    "    get_tz_info(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
