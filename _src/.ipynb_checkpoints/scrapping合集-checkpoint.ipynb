{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 从web抓取信息\n",
    "\n",
    "起源于《Python编程快速上手——让繁琐工作自动化》中的第11章“从web抓取信息”，也有自己生发的及其他书的内容。\n",
    "\n",
    "几个重要的库：  \n",
    "\n",
    "- 1. `webbrowser`\n",
    "- 2. `requests`\n",
    "- 3. `BeautifulSoup`  \n",
    "- 4. `selenium`\n",
    "\n",
    "另外还有`urllib`，但AI Sweigart 说让我忘了这个库，意思是很不好用。\n",
    "\n",
    "---\n",
    "附录：关于HTML\n",
    "\n",
    "**苹果系统中，command+option+I，可以打开或关闭开发者工具，和Windows上的F12是一样的。**  \n",
    "\n",
    "作者建议，**不要用正则表达式来解析HTML**。例如昨天遇到的将class写在a标签中间的那种，对于html来说仍然有效，用正则来预估所有的情况则会非常繁琐。专门用来解析html的模块，例如beautifulsoup，将更不容易出错。[html - RegEx match open tags except XHTML self-contained tags - Stack Overflow](https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454#1732454)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ipython输出各行结果\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  `webbrowser`模块\n",
    "\n",
    "`webbrowser`这个模块，可以直接打开网址。\n",
    "\n",
    "先生成网址，再用webbrowser打开，适合网址有规律的情况。哪些情况适用于生成网址再打开检查的情况？\n",
    "\n",
    "- 动态网址  \n",
    "\n",
    "    - 查询类：内容参数组成新网址，如Google Map\n",
    "    - 解析类：从某段文字中解析出需要的新网址\n",
    "    \n",
    "- 页面内容确认\n",
    "\n",
    "    - 编号类：某种无序数列组成新网址，如小说网站晋江\n",
    "    - 已经获得，手动打开麻烦"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 打开小说页面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "for i in range(3000011, 3000020):\n",
    "    url = 'http://www.jjwxc.net/onebook.php?novelid=' + str(i)\n",
    "    webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 打开Google地图上单个城市"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import webbrowser\n",
    "import re\n",
    "\n",
    "def mapit(address):\n",
    "    address = re.compile(r'\\s+').sub('+',address) # 这里“+”之前不需要转义符\\\n",
    "    url = 'http://www.google.com/maps/place/' + address\n",
    "    return webbrowser.open(url)\n",
    "\n",
    "address = 'Wuxi,   Jiangsu,   China'    \n",
    "mapit(address)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 批量打开Google地图的城市群"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser\n",
    "\n",
    "def mapcities(cities):\n",
    "    address = []\n",
    "    for city in cities:\n",
    "        address.append(city + ', Jiangsu, China')\n",
    "    for a in address:\n",
    "        a = re.compile(r'\\s+').sub('+',a)\n",
    "        url = 'http://www.google.com/maps/place/' + a\n",
    "        webbrowser.open(url)\n",
    "        \n",
    "cities = ['Wuxi', 'Suzhou', 'Xuzhou', 'Zhengjiang', 'Taizhou']\n",
    "mapcities(cities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 批量打开简书笔记\n",
    "\n",
    "复制下面这段网址（从workflowy来），用下面的程序批量打开：\n",
    "\n",
    "    [每天一本书 -《思考线》](https://www.jianshu.com/p/ee5e1c32f97d)\n",
    "\t[创意变为现实的最佳方法——《思考线》读后感](https://www.jianshu.com/p/c2132f7e02ae)\n",
    "\t[读思考线 ](https://www.jianshu.com/p/52e2e4dbb08c)\n",
    "\t[极具说服力的书（思考线：让你的创意变为现实的最佳方法](https://book.douban.com/review/7747085/)\n",
    "\t[思考线·思维导图.png (3104×1802)](https://upload-images.jianshu.io/upload_images/14183687-4b46af1a5294edfd.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import webbrowser, pyperclip, re\n",
    "\n",
    "mdUrls = pyperclip.paste().replace('\\t','').split('\\n')\n",
    "urlRegex = re.compile(r'\\((http.*)\\)')\n",
    "for mdUrl in mdUrls:\n",
    "    url = urlRegex.search(mdUrl).group(1)\n",
    "    webbrowser.open(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. `requests`模块\n",
    "\n",
    "requests文档地址：[Requests: HTTP for Humans™ — Requests 2.21.0 documentation](https://requests.readthedocs.io/en/master/)\n",
    "\n",
    "`requests.get(url)`:  \n",
    "\n",
    "- 类型是`requests.models.Response`\n",
    "- 参数text\n",
    "- 参数headers是类似于字典（字典有`dict.get(key)`返回value的语法）的结构：`requests.structures.CaseInsensitiveDict`，它的键不区分大小写。真正的字典键是区分大小写的。\n",
    "- 参数status_code\n",
    "- 参数encoding\n",
    "- 方法raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "requests.models.Response"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'ISO-8859-1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "179378"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "\"\\ufeffThe Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\\r\\n\\r\\n\\r\\n*******************************************************************\\r\\nTHIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES PRODUCED AT A\\r\\nTIME WHEN PROOFING METHODS AND TOO\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'text/plain; charset=utf-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "{'Server': 'AliyunOSS', 'Date': 'Fri, 01 Nov 2019 01:39:49 GMT', 'Content-Type': 'text/html', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'Vary': 'Accept-Encoding', 'x-oss-request-id': '5DBB8CE55C74183036984F90', 'Last-Modified': 'Thu, 31 Oct 2019 02:27:20 GMT', 'x-oss-object-type': 'Normal', 'x-oss-hash-crc64ecma': '4434422554274640270', 'x-oss-storage-class': 'Standard', 'Content-MD5': 'l2yxdfF3i/9yvmsmNVa0SA==', 'x-oss-server-time': '18', 'Content-Encoding': 'gzip'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "HTTPError",
     "evalue": "404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-21c72c8a0df0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0murl3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://inventwithpython.com/page_that_does_not_exist'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0mres3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m \u001b[0mres3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    939\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 940\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    941\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    942\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import requests\n",
    "import chardet\n",
    "\n",
    "url0 = 'http://www.gutenberg.org/cache/epub/1112/pg1112.txt'\n",
    "url1 = 'http://example.webscraping.com'\n",
    "url2 = 'http://www.engine3d.com'\n",
    "\n",
    "res0 = requests.get(url0)\n",
    "res1 = requests.get(url1)\n",
    "headers = {'user-agent': 'my-app/0.0.1'}\n",
    "res2 = requests.get(url2, headers = headers)\n",
    "\n",
    "# res的基本情况\n",
    "type(res0)\n",
    "res0.status_code == requests.codes.ok\n",
    "res2.encoding\n",
    "\n",
    "# 看res的文本\n",
    "len(res0.text)\n",
    "res0.text[:250]\n",
    "\n",
    "# 看res的headers\n",
    "res0.headers.get('content-type')\n",
    "res2.headers\n",
    "res2.headers.get('user-agent')\n",
    "\n",
    "# 不存在的网址，看res反应\n",
    "url3 = 'http://inventwithpython.com/page_that_does_not_exist'\n",
    "res3 = requests.get(url3)\n",
    "res3.raise_for_status()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 能运行起来的第一段程序："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url0的下载结果： \n",
      "\n",
      "Webpage is download successfully.\n",
      "The beginning texts here:  \n",
      "\n",
      "\n",
      "﻿The Project Gutenberg EBook of Romeo and Juliet, by William Shakespeare\n",
      "\n",
      "\n",
      "*******************************************************************\n",
      "THIS EBOOK WAS ONE OF PROJECT GUTENBERG'S EARLY FILES\n",
      "url3的下载结果： \n",
      "\n",
      "There was a problem: \n",
      "404 Client Error: Not Found for url: http://inventwithpython.com/page_that_does_not_exist\n"
     ]
    }
   ],
   "source": [
    "def download(url):\n",
    "    res = requests.get(url)\n",
    "    try:\n",
    "        res.raise_for_status()\n",
    "        html = res.text\n",
    "        print('Webpage is download successfully.'+'\\n'+'The beginning texts here:  '+'\\n\\n')\n",
    "        print(html[:200])\n",
    "    except Exception as exc :\n",
    "        print('There was a problem: \\n%s' %exc)\n",
    "print('url0的下载结果： \\n')\n",
    "download(url0)\n",
    "print('url3的下载结果： \\n')\n",
    "download(url3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 保存文件到本地"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "79380"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "\n",
    "res = requests.get(url0)\n",
    "res.raise_for_status()\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'RomeoAndJuliet.txt')\n",
    "playFile = open(path, 'wb')\n",
    "for chunk in res.iter_content(100000):\n",
    "    playFile.write(chunk)\n",
    "playFile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 源代码的编码问题\n",
    "\n",
    "关于Unicode编码的知识：[The Absolute Minimum Every Software Developer Absolutely, Positively Must Know About Unicode and Character Sets (No Excuses!) – Joel on Software](https://www.joelonsoftware.com/2003/10/08/the-absolute-minimum-every-software-developer-absolutely-positively-must-know-about-unicode-and-character-sets-no-excuses/)\n",
    "\n",
    "```\n",
    "encode_type = chardet.detect(html)\n",
    "html = html.decode(encode_type['encoding'])\n",
    "```\n",
    "\n",
    "这里不是靠这两句解决问题的。已经测试过，html是str类型的。\n",
    "\n",
    "用wb模式打开文件，写入内容是 `res.iter_content(100000)`  \n",
    "作者说，使用iter_content是为了确保requests模块即使在**下载巨大**的文件时也**不会消耗太多**内存。\n",
    "\n",
    "**The chunk size is the number of bytes it should read into memory. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method iter_content in module requests.models:\n",
      "\n",
      "iter_content(chunk_size=1, decode_unicode=False) method of requests.models.Response instance\n",
      "    Iterates over the response data.  When stream=True is set on the\n",
      "    request, this avoids reading the content at once into memory for\n",
      "    large responses.  The chunk size is the number of bytes it should\n",
      "    read into memory.  This is not necessarily the length of each item\n",
      "    returned as decoding can take place.\n",
      "    \n",
      "    chunk_size must be of type int or None. A value of None will\n",
      "    function differently depending on the value of `stream`.\n",
      "    stream=True will read data as it arrives in whatever size the\n",
      "    chunks are received. If stream=False, data is returned as\n",
      "    a single chunk.\n",
      "    \n",
      "    If decode_unicode is True, content will be decoded using the best\n",
      "    available encoding based on the response.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(res.iter_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. `BeautifulSoup`模块\n",
    "\n",
    "[BeautifulSoup高级应用 之 CSS selectors /CSS 选择器 - Winterto1990的博客 - CSDN博客](https://blog.csdn.net/Winterto1990/article/details/47808949)\n",
    "\n",
    "soup的两类来源：\n",
    "\n",
    "- 1. 从网页获取\n",
    "- 2. 从本地获取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 尝试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "bs4.BeautifulSoup"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "# soup的两类来源：\n",
    "# 1. 从网页获取\n",
    "url = 'https://www.tripadvisor.cn/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "example1Soup = bs4.BeautifulSoup(res.text)\n",
    "type(example1Soup)\n",
    "\n",
    "# 2. 从本地获取\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'NoStarch.html')\n",
    "example2File = open(path)\n",
    "example2Soup = bs4.BeautifulSoup(example2File.read()) #.read()加了没加没区别。\n",
    "type(example2Soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>TripAdvisor(猫途鹰) - 全球旅游点评,酒店/景点/餐厅,真实旅客评论</title>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<img alt=\"TripAdvisor(猫途鹰)\" class=\"brand-header-Logo__resizeImg--15ZcW\" src=\"https://cc.ddcdn.com/img2/langs/zh_CN/branding/rebrand/TA_logo_primary.svg\"/>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<img alt=\"TripAdvisor(猫途鹰)\" class=\"brand-header-Logo__resizeImg--15ZcW\" src=\"https://cc.ddcdn.com/img2/langs/zh_CN/branding/rebrand/TA_logo_primary.svg\"/>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\ntripSoup.select(\\'input[type=\"radio\"]\\')\\ntripSoup.select_one(\\'input[type=\"radio\"]\\')\\ntripSoup.select(\\'#popularDestinations > div.section > ul.regionContent > li.active > ul > li:nth-child(1) > a > span.thumbCrop > img\\')\\nimages = tripSoup.select(\\'#popularDestinations > div.section > ul.regionContent > li.active > ul > li > a > span.thumbCrop > img\\')\\ntitles = tripSoup.select(\\'#popularDestinations > div.section > ul.regionContent > li.active > ul > li > div.title\\')\\ninfo = []\\nfor title,image in zip(titles, images):\\n    data = {\\n            \\'title\\':((title.get_text()).replace(\\'\\n\\',\\'\\')).replace(\\'游记指南\\',\\'\\'),\\n            \\'image\\':image.get(\\'src\\')\\n        }\\n    info.append(data)\\ninfo\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "['height:100%', 'width:100%', 'background-size:cover', 'background-image:none']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lxml解析 \n",
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "\n",
    "url = 'https://www.tripadvisor.cn/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "html = res.text\n",
    "tripSoup = bs4.BeautifulSoup(html, 'lxml')\n",
    "\n",
    "tripSoup.select('title')\n",
    "tripSoup.select_one('a>img')\n",
    "tripSoup.select_one('a img')\n",
    "# 网页变了，以下代码失效。\n",
    "'''\n",
    "tripSoup.select('input[type=\"radio\"]')\n",
    "tripSoup.select_one('input[type=\"radio\"]')\n",
    "tripSoup.select('#popularDestinations > div.section > ul.regionContent > li.active > ul > li:nth-child(1) > a > span.thumbCrop > img')\n",
    "images = tripSoup.select('#popularDestinations > div.section > ul.regionContent > li.active > ul > li > a > span.thumbCrop > img')\n",
    "titles = tripSoup.select('#popularDestinations > div.section > ul.regionContent > li.active > ul > li > div.title')\n",
    "info = []\n",
    "for title,image in zip(titles, images):\n",
    "    data = {\n",
    "            'title':((title.get_text()).replace('\\n','')).replace('游记指南',''),\n",
    "            'image':image.get('src')\n",
    "        }\n",
    "    info.append(data)\n",
    "info\n",
    "'''\n",
    "# 不知道为什么读不到背景图像的url\n",
    "tripSoup.select('a>div>ul>li>div')[0]['style'].split(';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome, Prashanth!\n",
      "https://www.joelonsoftware.com/2019/09/24/announcing-stack-overflows-new-ceo/\n",
      "The next CEO of Stack Overflow\n",
      "https://www.joelonsoftware.com/2019/03/28/the-next-ceo-of-stack-overflow/\n",
      "Things You Should Never Do, Part I\n",
      "Strategy Letter I: Ben and Jerry’s vs. Amazon\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://i1.wp.com/www.joelonsoftware.com/wp-content/uploads/2016/12/Pong.png?w=730&ssl=1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://www.joelonsoftware.com/'\n",
    "res = requests.get(url)\n",
    "htmlSoup = bs4.BeautifulSoup(res.text)\n",
    "\n",
    "import pprint\n",
    "for i in range(2):\n",
    "    # pprint.pprint(htmlSoup.select('div>p>span>a')[i].attrs)\n",
    "    # print(htmlSoup.select('div>p>span>a')[i].get('href'))\n",
    "    # pprint.pprint(htmlSoup.select('header>h2>a')[i].attrs)\n",
    "    print(htmlSoup.select('header>h2>a')[i].text)\n",
    "    print(htmlSoup.select('header>h2>a')[i].get('href'))\n",
    "for i in range(2):\n",
    "    print(htmlSoup.select('div>ul>li>a')[i].text)\n",
    "    \n",
    "htmlSoup.select_one('img')['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<title>CSDN博客-专业IT技术发表平台</title>]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<a href=\"/\">推荐</a>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\t\\t\\t\\t\\tCSDN产品公告第2期：博客支持视频、专栏文章拖拽排序、APP霸王课来袭……\\t\\t\\t\\t\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4\n",
    "\n",
    "url = 'https://blog.csdn.net' \n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "soup.select('title')\n",
    "# soup.select('div nth-of-type(0)') 在ipython中不好用\n",
    "# soup.select('body a')[:3]\n",
    "# soup.select('div>ul>li.active')\n",
    "# soup.select('.carousel-caption, p.name')\n",
    "soup.select('a[href]')[0]\n",
    "soup.select(\"a[href$='102605809']\")[1].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 下载古腾堡中文书\n",
    "\n",
    "突然想下载古腾堡的书下来。挑选中文的试试吧。  \n",
    "\n",
    "【下载】\n",
    "1. 通过主页面提取出txt的url\n",
    "2. 使用上面的功能下载这些。\n",
    "\n",
    "【繁体转简体】\n",
    "找到对应的库 \n",
    "\n",
    "> 不需要什么安装方法，只需要把这两个文件下载下来，保存到与代码同一目录下即可\n",
    "```\n",
    "https://raw.githubusercontent.com/skydark/nstools/master/zhtools/langconv.py  \n",
    "https://raw.githubusercontent.com/skydark/nstools/master/zhtools/zh_wiki.py\n",
    "```\n",
    "\n",
    "#### 3.2.1 下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载txt\n",
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "url = 'https://www.gutenberg.org/browse/languages/zh'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "fictionSoup = bs4.BeautifulSoup(res.text)\n",
    "fictionList = fictionSoup.select(\"li.pgdbetext a[href^='/ebooks']\")\n",
    "for i in range(len(fictionList)):\n",
    "    fileName = fictionList[i].get_text()    \n",
    "    if '/' in fileName:\n",
    "        fileName = fileName.replace('/', ' ')\n",
    "    if '\\\\' in fileName:\n",
    "        fileName = fileName.replace('\\\\', ' ')\n",
    "    file = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', fileName +'.txt')\n",
    "    if os.path.exists(file):\n",
    "        fileName = fileName + '_new'\n",
    "        file = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', fileName +'.txt')\n",
    "    fictionID = (fictionList[i].get('href')).split('/')[2]\n",
    "    fictionUrl = 'https://www.gutenberg.org/files/' + fictionID + '/' + fictionID + '-0.txt'\n",
    "    with open(file ,'wb') as fictionFile:\n",
    "        resFiction = requests.get(fictionUrl)\n",
    "        for chunk in resFiction.iter_content(100000):\n",
    "            fictionFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一共下载了475本txt文本格式的电子书。  \n",
    "\n",
    "现在看txt格式的都可以下载，试试其他格式的：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载epub\n",
    "\n",
    "file = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', '玉樓春.epub')\n",
    "url = 'https://www.gutenberg.org/ebooks/25422.epub.noimages?session_id=3c29b07a963878c5cd004f277b6d1d0adb08d623'\n",
    "with open(file ,'wb') as fictionFile:\n",
    "        resFiction = requests.get(url)\n",
    "        for chunk in resFiction.iter_content(100000):\n",
    "            fictionFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.2 转换简繁体  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'玉楼春'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langconv import *\n",
    "\n",
    "sentence = '玉樓春'\n",
    "Converter('zh-hans').convert(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要想批量转换，需要：  \n",
    "\n",
    "1. 转换文件名  \n",
    "2. 建立新文件——考虑到有些文件名已经是简体，保险的方法就是再建立一个简体文件夹。  \n",
    "3. 读取文件中的内容\n",
    "4. 转换文件内容  \n",
    "5. 写入新文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from langconv import Converter\n",
    "\n",
    "pathFanti = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'fanti')\n",
    "pathJianti = os.path.join(os.path.dirname(os.getcwd()), 'files', 'fiction', 'jianti')\n",
    "filesFanti = os.listdir(pathFanti) #把这个目录下的所有文件都读出来\n",
    "for fileName in filesFanti:\n",
    "    if fileName.split('.')[-1] != 'txt':\n",
    "        filesFanti.remove(fileName)\n",
    "for fileNameFanti in filesFanti:\n",
    "    fileNameJianti = Converter('zh-hans').convert(fileNameFanti)\n",
    "    with open(os.path.join(pathJianti, fileNameJianti), 'w') as fileJianti:\n",
    "        with open(os.path.join(pathFanti, fileNameFanti)) as fileFanti:\n",
    "            contentFanti = fileFanti.readlines()\n",
    "            for sentenceFanti in contentFanti:\n",
    "                sentenceJianti = Converter('zh-hans').convert(sentenceFanti)\n",
    "                fileJianti.write(sentenceJianti)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了一个打不开的，其他都转换成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Google自动查询"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Googling...\n"
     ]
    }
   ],
   "source": [
    "# _*_coding:utf-8_*_\n",
    "import requests, bs4, webbrowser,re\n",
    "\n",
    "def googleit(query):\n",
    "    \n",
    "    #打开查询结果页面\n",
    "    if '' in query:\n",
    "        query = re.compile(r'\\s+').sub('+', query)\n",
    "    url = 'http://www.google.com/search?q=' + query\n",
    "    print('Googling...')\n",
    "    headers = {'User-Agent':'Mozilla/8.0 (compatible; MSIE 8.0; Windows 7)'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    \n",
    "    #选择结果页面\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    linkElems = soup.select('.r a')\n",
    "    \n",
    "    #打开前5个页面\n",
    "    numOpen = min(5, len(linkElems))\n",
    "    for i in range(numOpen):\n",
    "        webbrowser.open('http://google.com' + linkElems[i].get('href'))\n",
    "        # print(linkElems[i].get('href'))\n",
    "query = 'python webscraping'\n",
    "googleit(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 下载xkcd漫画\n",
    "\n",
    "这是第一次尝试下载图片。先开始是教材上的程序，后来自己又重新写了一下。\n",
    "\n",
    "#### 3.4.1 标准程序"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "That is wrong. Downloading image is http://www.xkcd.com/2067/asset/challengers_header.png ...\n",
      "Could not find comic image.\n",
      "Could not find comic image.\n",
      "That is wrong. Downloading image is http://www.xkcd.com/1525/bg.png ...\n",
      "Could not find comic image.\n",
      "Could not find comic image.\n"
     ]
    }
   ],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "import requests, bs4,os\n",
    "\n",
    "url = 'http://xkcd.com'\n",
    "# os.makedirs('xkcd', exist_ok=True)\n",
    "while not url.endswith('#'):\n",
    "    #下载网页\n",
    "    # print('Downloading page %s ...'%url)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    \n",
    "    #寻找漫画\n",
    "    comicElem = soup.select('#comic img')\n",
    "    if comicElem == []:\n",
    "        print('Could not find comic image.')\n",
    "    else:\n",
    "        try:\n",
    "            comicUrl = 'http:'+ comicElem[0].get('src')\n",
    "            # print('Downloading image is %s ...'%comicUrl)\n",
    "            res = requests.get(comicUrl)\n",
    "            res.raise_for_status()  \n",
    "        except:\n",
    "            comicUrl = 'http://www.xkcd.com'+ comicElem[0].get('src')\n",
    "            print('That is wrong. Downloading image is %s ...'%comicUrl)    \n",
    "            res = requests.get(comicUrl)\n",
    "            res.raise_for_status() \n",
    "\n",
    "    #保存漫画\n",
    "    # imageFile = open(os.path.join('xkcd', os.path.basename(comicUrl)), 'wb')\n",
    "    imageFile = open(os.path.dirname(os.getcwd()) + '/files/xkcd/' + os.path.basename(comicUrl), 'wb')\n",
    "    for chunk in res.iter_content(100000):\n",
    "        imageFile.write(chunk)\n",
    "    imageFile.close() \n",
    "\n",
    "    #找前一张漫画\n",
    "    prevLink = soup.select('a[rel=\"prev\"]')[0]\n",
    "    url = 'http://xkcd.com' + prevLink.get('href')\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading image is http://imgs.xkcd.com/comics/pie_charts.png ...\n"
     ]
    }
   ],
   "source": [
    "url = 'https://xkcd.com/2031/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "comicElem = soup.select('#comic img')\n",
    "try:\n",
    "    comicUrl = 'http:'+ comicElem[0].get('src')\n",
    "    print('Downloading image is %s ...'%comicUrl)\n",
    "    res = requests.get(comicUrl)\n",
    "    res.raise_for_status()  \n",
    "except:\n",
    "    comicUrl = 'http://www.xkcd.com'+ comicElem[0].get('src')\n",
    "    print('That is wrong. Downloading image is %s ...'%comicUrl)    \n",
    "    res = requests.get(comicUrl)\n",
    "    res.raise_for_status() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明白了，这里用的是canvas，不是img。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2031和1813都出现的错误：  \n",
    "\n",
    "SysCallError                              Traceback (most recent call last)\n",
    "\n",
    "SSLError: HTTPSConnectionPool(host='xkcd.com', port=443): Max retries exceeded with url: /1813/ (Caused by SSLError(SSLError(\"bad handshake: SysCallError(-1, 'Unexpected EOF')\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  3.4.2 重新下载xkcd漫画\n",
    "\n",
    "在知道了漫画作者门罗就是《万物解释者》作者后，决定重新下载xkcd漫画。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import os\n",
    "\n",
    "url = 'https://xkcd.com/1'\n",
    "while True:\n",
    "    # 获取当前页面的漫画地址\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    try:\n",
    "        # 当前页面存在漫画的静态图片地址\n",
    "        srcComic = soup.select_one('#comic img').get('src')\n",
    "        urlComic = 'https:'+ srcComic\n",
    "    except:\n",
    "        # 没有漫画地址就直接找下一页\n",
    "        srcPrev = soup.select_one(\".comicNav a[rel='prev']\").get('href')\n",
    "        print(srcPrev)\n",
    "        url = 'https://xkcd.com' + srcPrev\n",
    "        continue\n",
    "    \n",
    "    # 存储当前漫画到本地\n",
    "    fileName = srcComic.split('/')[-1]\n",
    "    filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'xkcd', fileName)\n",
    "    with open(filePath, 'wb') as comicFile:\n",
    "        resComic = requests.get(urlComic)\n",
    "        for chunk in resComic.iter_content(100000):\n",
    "            comicFile.write(chunk)\n",
    "    \n",
    "    # 获取前一页的页面地址\n",
    "    srcPrev = soup.select_one(\".comicNav a[rel='prev']\").get('href')\n",
    "    # 到了第一幅漫画的页面\n",
    "    if srcPrev == '#':\n",
    "        break\n",
    "    print(srcPrev)\n",
    "    url = 'https://xkcd.com' + srcPrev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有问题的：2198, 2067，1663，1608，1525，1416，1350  \n",
    "\n",
    "2198、1663、1608、1525、1416都是游戏，页面有交互，非普通静态图片。1416会放大，嵌入了一个框架网页。1350无内容。\n",
    "\n",
    "2067会放大，其中有链接，也不是普通图片。错误显示为：  \n",
    "\n",
    "```\n",
    "MissingSchema: Invalid URL 'https:/2067/asset/challengers_header.png': No schema supplied. Perhaps you meant http://https:/2067/asset/challengers_header.png?\n",
    "```\n",
    "\n",
    "1052-878之间一次性完成，无意外。874-787一次性完成。691-483一次性完成。446-250一次性完成\n",
    "\n",
    "超时的错误为：  \n",
    "\n",
    "```\n",
    "SSLError: HTTPSConnectionPool(host='xkcd.com', port=443): Max retries exceeded with url: /240/ (Caused by SSLError(SSLError(\"bad handshake: SysCallError(60, 'ETIMEDOUT')\")))\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "1. 需要写一个try-except，出了意外，直接找prev按钮继续走下去。 \n",
    "   try-except无法解决2067的问题。\n",
    "2. 对于timeout怎么应对？\n",
    "\n",
    "目前程序的三个问题：  \n",
    "\n",
    "1. try-except无法解决2067的问题  \n",
    "2. timeout无法解决  \n",
    "3. 下载速度太慢，平均每小时只能下载200-300张漫画，全部2200张漫画需要七八个小时才能下完。  \n",
    "\n",
    "问题三可以用多线程来解决。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 单独下载一副漫画\n",
    "\n",
    "url = 'https://xkcd.com/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text)\n",
    "srcComic = soup.select_one('#comic img').get('src')\n",
    "urlComic = 'https:'+ srcComic\n",
    "\n",
    "# 存储当前漫画到本地\n",
    "fileName = srcComic.split('/')[-1]\n",
    "filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'xkcd', fileName)\n",
    "with open(filePath, 'wb') as comicFile:\n",
    "    resComic = requests.get(urlComic)\n",
    "    for chunk in resComic.iter_content(100000):\n",
    "        comicFile.write(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4.3 多线程下载xkcd漫画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Download End.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import bs4\n",
    "import threading\n",
    "import os\n",
    "\n",
    "def downloadComic(startComic, endComic):\n",
    "    for comicID in range(startComic, endComic):\n",
    "        # 获取漫画地址\n",
    "        res = requests.get('https://xkcd.com/%s'%str(comicID))\n",
    "        soup = bs4.BeautifulSoup(res.text)\n",
    "        comicElem = soup.select('#comic img')\n",
    "        if comicElem == []:\n",
    "            print('Could not find comic img: %s'%str(comicID))\n",
    "        else:\n",
    "            srcComic = comicElem[0].get('src')\n",
    "            urlComic = 'https:'+ srcComic\n",
    "            # 存储到本地\n",
    "            fileName = str(comicID) + '-' + srcComic.split('/')[-1]\n",
    "            filePath = os.path.join(os.path.dirname(os.getcwd()), 'files', 'xkcd', fileName)\n",
    "            if os.path.exists(filePath):\n",
    "                continue\n",
    "            with open(filePath, 'wb') as comicFile:\n",
    "                resComic = requests.get(urlComic)\n",
    "                for chunk in resComic.iter_content(100000):\n",
    "                    comicFile.write(chunk)\n",
    "\n",
    "downloadTreads = []\n",
    "for i in range(1, 81, 10):\n",
    "    downloadTread = threading.Thread(target=downloadComic, args=[i, i+10])\n",
    "    downloadTreads.append(downloadTread)\n",
    "    downloadTread.start()\n",
    "for downloadTread in downloadTreads:\n",
    "    downloadTread.join()\n",
    "print('Download End.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了顺利地在所有线程结束后打印‘Download End.’，必须所有线程都没有崩溃才行。timeout一旦出现，这个程序就处于永远结束不了的状态了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 下载极客漫画"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, os\n",
    "\n",
    "for i in range(1,6):\n",
    "    url = 'https://linux.cn/talk/comic/index.php?page=' + str(i)\n",
    "    headers = {'User-Agent':'Mozilla/8.0 (compatible; MSIE 8.0; Windows 7)'}\n",
    "    res = requests.get(url, headers=headers)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text)\n",
    "    for s in soup.select('h2 span[class=\"title\"] a'):\n",
    "        comicUrl = s.get('href')\n",
    "        comicRes = requests.get(comicUrl, headers=headers)\n",
    "        comicRes.raise_for_status()\n",
    "        comicSoup = bs4.BeautifulSoup(comicRes.text)\n",
    "        comicImageUrl = comicSoup.select('#article_content img')[0].get('src')\n",
    "        comicImageRes = requests.get(comicImageUrl, headers=headers)\n",
    "        imageFile = open(os.path.dirname(os.getcwd()) + '/files/jkmh/' + os.path.basename(comicImageUrl), 'wb')\n",
    "        for chunk in comicImageRes.iter_content(100000):\n",
    "            imageFile.write(chunk)\n",
    "        imageFile.close()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 玩豆瓣\n",
    "\n",
    "#### 3.6.1 豆瓣top250图书\n",
    "\n",
    "豆瓣有个top250的图书榜单，读取其中书名、作者和评分。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '红楼梦', 'author': '曹雪芹', 'score': '9.6'}\n",
      "{'title': '海贼王:ONEPIECE', 'author': '尾田荣一郎', 'score': '9.5'}\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml, re\n",
    "urls = []\n",
    "for i in range(0,250,25):\n",
    "    urls.append('https://book.douban.com/top250?start=' + str(i))\n",
    "info = []\n",
    "for url in urls:\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    html = res.text\n",
    "    doubanBookSoup = bs4.BeautifulSoup(html, 'lxml')\n",
    "    titles = doubanBookSoup.select('a[title]')\n",
    "    scores = doubanBookSoup.select('span[class=\"rating_nums\"]')\n",
    "    authors = doubanBookSoup.select('p.pl')\n",
    "    for title, author, score in zip(titles, authors, scores):\n",
    "        data = {\n",
    "            'title':((title.get_text()).replace('\\n','')).replace(' ','') ,\n",
    "            'author':re.compile(r'(\\[\\w+\\])?(\\w)+(·)?(\\w+)(·\\w+)?').search(author.get_text()).group(),\n",
    "            'score':score.get_text()\n",
    "        }\n",
    "        info.append(data)\n",
    "\n",
    "for i in info:\n",
    "    if 9.5<=float(i['score'])<9.7:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': '红楼梦', 'Rating': '9.6', 'Author': '[清] 曹雪芹 著 '}\n",
      "{'Title': '海贼王', 'Rating': '9.5', 'Author': '尾田荣一郎 '}\n"
     ]
    }
   ],
   "source": [
    "# 2019.11.01重新写一遍\n",
    "import requests\n",
    "import bs4\n",
    "import lxml\n",
    "import os\n",
    "\n",
    "topBooks = []\n",
    "for pageNum in range(0,250,25):\n",
    "    url = 'https://book.douban.com/top250?start=' + str(pageNum)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    for i in range(25):\n",
    "        info = soup.select('p.pl')[i].text\n",
    "        data = {\n",
    "            'Title':soup.select('.pl2 a')[i]['title'],\n",
    "            'Rating':soup.select('.rating_nums')[i].text,    \n",
    "            'Author': info.split('/')[0]\n",
    "        }\n",
    "        topBooks.append(data)\n",
    "for book in topBooks:\n",
    "    if float(book['Rating'])>=9.5:\n",
    "        print(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.6.2 豆瓣标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 单标签\n",
    "import requests, bs4, lxml\n",
    "import os, openpyxl, re, threading\n",
    "\n",
    "def saveContent(tag):\n",
    "    # 建立并保存初始的Excel表\n",
    "    wb = openpyxl.Workbook()\n",
    "    sheetTag = wb.create_sheet()\n",
    "    sheetTag.title = tag\n",
    "    sheetTag['A1'], sheetTag['B1'], sheetTag['C1'] = 'Label', 'Title', 'Author'\n",
    "    sheetTag['D1'], sheetTag['E1'], sheetTag['F1'] = 'Rating', 'Comments', 'Link' \n",
    "    path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'douban', 'tagsEdu.xlsx')\n",
    "    wb.save(path)\n",
    "    wb.close()\n",
    "    \n",
    "    # 初始url\n",
    "    urlTag = 'https://book.douban.com/tag/%E6%95%99%E8%82%B2'\n",
    "\n",
    "    # 读取数据\n",
    "    total = 0\n",
    "    while True:\n",
    "        resTag = requests.get(urlTag)\n",
    "        resTag.raise_for_status()\n",
    "        soupTag = bs4.BeautifulSoup(resTag.text, 'lxml')\n",
    "        num = len(soupTag.select('h2 a'))\n",
    "        if num == 0:\n",
    "            break\n",
    "        for i in range(num):\n",
    "            sheetTag['A'+str(i+2+total)] = soupTag.select('h1')[0].text.split(': ')[1]\n",
    "            # print(soupTag.select('h2 a')[i].get('title'))\n",
    "            sheetTag['B'+str(i+2+total)] = soupTag.select('h2 a')[i].get('title')\n",
    "            sheetTag['C'+str(i+2+total)] = re.compile(r'\\s+').sub('', soupTag.select('div.pub')[i].text).split('/')[0]\n",
    "            sheetTag['F'+str(i+2+total)] = soupTag.select('h2 a')[i].get('href') \n",
    "            if '少于10人评价' in soupTag.select('.clearfix .pl')[i].text or '无人评价' in soupTag.select('.clearfix .pl')[i].text:\n",
    "                sheetTag['D'+str(i+2+total)] = ''\n",
    "                sheetTag['E'+str(i+2+total)] = 0\n",
    "            else:\n",
    "                sheetTag['D'+str(i+2+total)] = float(soupTag.select('.info .clearfix')[i].select('.rating_nums')[0].text)\n",
    "                sheetTag['E'+str(i+2+total)] = int(re.compile(r'\\d+').search(soupTag.select('.clearfix .pl')[i].text).group(0))\n",
    "        total = total + num\n",
    "        urlTag = 'https://book.douban.com' + soupTag.select('.next link')[0].get('href')\n",
    "    # 保存数据\n",
    "    wb.save(path)\n",
    "    wb.close()\n",
    "saveContent('教育')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 多标签\n",
    "import requests, bs4, lxml\n",
    "import os, openpyxl, re, threading\n",
    "\n",
    "url = 'https://book.douban.com/tag/'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "wb = openpyxl.Workbook()\n",
    "tags = []\n",
    "for tagID in range(len(soup.select('.tagCol a'))):\n",
    "    urlTag = 'https://book.douban.com' + soup.select('.tagCol a')[tagID].get('href')   \n",
    "    sheetTag = wb.create_sheet()\n",
    "    tag = soup.select('.tagCol a')[tagID].text\n",
    "    sheetTag.title = tag\n",
    "    tags.append(tag)\n",
    "    sheetTag['A1'], sheetTag['B1'], sheetTag['C1'] = 'Label', 'Title', 'Author'\n",
    "    sheetTag['D1'], sheetTag['E1'], sheetTag['F1'] = 'Rating', 'Comments', 'Link'  \n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'douban', 'tags.xlsx')\n",
    "wb.save(path)\n",
    "wb.close()\n",
    "\n",
    "def saveContent(tag):\n",
    "    urlTag = 'https://book.douban.com/tag/' + tag\n",
    "    sheetTag = wb[tag]\n",
    "    total = 0\n",
    "    while True:\n",
    "        resTag = requests.get(urlTag)\n",
    "        resTag.raise_for_status()\n",
    "        soupTag = bs4.BeautifulSoup(resTag.text, 'lxml')\n",
    "        num = len(soupTag.select('h2 a'))\n",
    "        if num == 0:\n",
    "            break\n",
    "        for i in range(num):\n",
    "            soupTag.select('h1')[0].text.split(': ')[1]\n",
    "            sheetTag['A'+str(i+2+total)] = soupTag.select('h1')[0].text.split(': ')[1]\n",
    "            sheetTag['B'+str(i+2+total)] = soupTag.select('h2 a')[i].get('title')\n",
    "            sheetTag['C'+str(i+2+total)] = re.compile(r'\\s+').sub('', soupTag.select('div.pub')[i].text).split('/')[0]\n",
    "            sheetTag['F'+str(i+2+total)] = soupTag.select('h2 a')[i].get('href') \n",
    "            if '少于10人评价' in soupTag.select('.clearfix .pl')[i].text or '无人评价' in soupTag.select('.clearfix .pl')[i].text:\n",
    "                sheetTag['D'+str(i+2+total)] = ''\n",
    "                sheetTag['E'+str(i+2+total)] = 0\n",
    "            else:\n",
    "                sheetTag['D'+str(i+2+total)] = soupTag.select('.info .clearfix')[i].select('.rating_nums')[0].text\n",
    "                sheetTag['E'+str(i+2+total)] = int(re.compile(r'\\d+').search(soupTag.select('.clearfix .pl')[i].text).group(0))\n",
    "        total = total + num\n",
    "        urlTag = 'https://book.douban.com' + soupTag.select('.next link')[0].get('href')\n",
    "    path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'douban', 'tags.xlsx')\n",
    "    wb.save(path)\n",
    "    wb.close()\n",
    "\n",
    "saveThreads = []\n",
    "for i in [10, 12] :\n",
    "    saveThread = threading.Thread(target=saveContent, args=[tags[i]])\n",
    "    saveThreads.append(saveThread)\n",
    "    saveThread.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3.7 赵雅芝贴吧内容读取 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.1 存入字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': '【典雅一生】《新白娘子传奇》截图帖（不定时更新）', 'replies': '2474', 'link': 'http://tieba.baidu.com/p/4983620050'}\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml\n",
    "\n",
    "info = []\n",
    "for i in range(0, 20000, 50):\n",
    "    url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E9%9B%85%E8%8A%9D&ie=utf-8&pn=' + str(i)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    yazhiSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    titles = yazhiSoup.select('a[class=\"j_th_tit\"]')\n",
    "    replies = yazhiSoup.select('span[class=\"threadlist_rep_num center_text\"]')\n",
    "    for title, reply in zip(titles, replies):\n",
    "        data = {\n",
    "            \"title\":title.get_text(),\n",
    "            \"replies\":reply.get_text(),\n",
    "            \"link\":'http://tieba.baidu.com' + title['href']\n",
    "        }\n",
    "        info.append(data)\n",
    "\n",
    "for i in info:\n",
    "    if '新白娘子传奇' in i['title'] and int(i['replies'])>2000:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.2 存入text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import requests, bs4, lxml, os\n",
    "\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'yazhitieba', 'yazhi.txt')\n",
    "with open(path, 'w+') as file_txt:\n",
    "    file_txt.write('-------------------Title-----------------replies---------------link--------------\\n')\n",
    "    for i in range(0, 1000, 50):\n",
    "        url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E9%9B%85%E8%8A%9D&ie=utf-8&pn=' + str(i)\n",
    "        res = requests.get(url)\n",
    "        res.raise_for_status()\n",
    "        yazhiSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "        titles = yazhiSoup.select('a[class=\"j_th_tit\"]')\n",
    "        replies = yazhiSoup.select('span[class=\"threadlist_rep_num center_text\"]')\n",
    "        for title, reply in zip(titles, replies):\n",
    "            data = {\n",
    "                \"title\":title.get_text(),\n",
    "                \"replies\":reply.get_text(),\n",
    "                \"link\":'http://tieba.baidu.com' + title['href']\n",
    "            }        \n",
    "            file_txt.write(data['title'] + '  ' + data['replies'] + '  ' + data['link'] + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.7.3 写入Excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated function get_active_sheet (Use the .active property).\n",
      "  after removing the cwd from sys.path.\n"
     ]
    }
   ],
   "source": [
    "import requests, bs4, lxml, openpyxl, os\n",
    "\n",
    "wb = openpyxl.Workbook()\n",
    "sheet = wb.get_active_sheet()\n",
    "sheet.title = 'Tieba'\n",
    "sheet['A1'] = 'Title'\n",
    "sheet['B1'] = 'Replies'\n",
    "sheet['C1'] = 'Link'\n",
    "total = 0\n",
    "\n",
    "for i in range(0, 20000, 50):\n",
    "    url = 'http://tieba.baidu.com/f?kw=%E8%B5%B5%E9%9B%85%E8%8A%9D&ie=utf-8&pn=' + str(i)\n",
    "    res = requests.get(url)\n",
    "    res.raise_for_status()\n",
    "    yazhiSoup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "    titles = yazhiSoup.select('a[class=\"j_th_tit\"]')\n",
    "    replies = yazhiSoup.select('span[class=\"threadlist_rep_num center_text\"]')\n",
    "    numbers = range(2+total, 52+total)\n",
    "    for title, reply, number in zip(titles, replies, numbers):\n",
    "        sheet['A'+str(number)] = title.get_text()\n",
    "        sheet['B'+str(number)] = reply.get_text()\n",
    "        sheet['C'+str(number)] = 'http://tieba.baidu.com' + title['href']\n",
    "    total = total + len(titles)\n",
    "path = os.path.join(os.path.dirname(os.getcwd()), 'files', 'yazhitieba', 'yazhi.xlsx')\n",
    "wb.save(path)\n",
    "wb.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. `selenium`模块\n",
    "\n",
    "折腾过程见教程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 尝试\n",
    "\n",
    "**如果browser赋值时打开的窗口关掉了，让browser.get(url)就会出错。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found <img> element with that class name!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "selenium.webdriver.remote.webelement.WebElement"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "url = 'http://inventwithpython.com'\n",
    "browser.get(url)\n",
    "\n",
    "try:\n",
    "    elem = browser.find_element_by_class_name('card-img-top')\n",
    "    print('Found <%s> element with that class name!' %(elem.tag_name))\n",
    "except:\n",
    "    print('Was not able to find an element with that name.')\n",
    "\n",
    "linkElem = browser.find_element_by_link_text('Read Online for Free')\n",
    "type(linkElem)\n",
    "\n",
    "linkElem.click()\n",
    "\n",
    "links = browser.find_elements_by_partial_link_text('free review copy')\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "真的把这个链接打开了。终于回忆起来**selenium的特性**是到哪里说哪里的话，点击进入哪个页面就只看哪个页面的元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "url = 'http://gmail.com'\n",
    "browser.get(url)\n",
    "\n",
    "# 找出input元素：\n",
    "classElem = browser.find_element_by_class_name('whsOnd')\n",
    "idElem = browser.find_element_by_id('identifierId')\n",
    "nameElem = browser.find_element_by_name('identifier')\n",
    "tagElem = browser.find_element_by_tag_name('input')\n",
    "classElem == idElem\n",
    "idElem == nameElem\n",
    "nameElem == tagElem\n",
    "\n",
    "tagsElem_div = browser.find_elements_by_tag_name('div')\n",
    "tagsElem_a = browser.find_elements_by_tag_name('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 登录gmail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 填写email帐号"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "import getpass\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "url = 'http://gmail.com'\n",
    "browser.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "email_input = browser.find_element_by_tag_name('input')\n",
    "email_input.clear()\n",
    "email_input.send_keys(getpass.getpass())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "试图毕其功于一役，将以上两个过程合在一起，失败。错误提示是：**元素不可见**。原因是无法打开gmail页面，也就找不到所谓元素了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 点击下一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "classElem_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 填写密码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "········\n"
     ]
    }
   ],
   "source": [
    "tagsElem_input = browser.find_elements_by_tag_name('input')\n",
    "password = tagsElem_input[2]\n",
    "password.send_keys(getpass.getpass())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.4 点击下一步"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "classElem_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下是很久不登录需要选择风格的画面。一般不会遇到。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "nameElem_btn = browser.find_element_by_name('welcome_dialog_next')\n",
    "nameElem_btn.click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeElem_btn = browser.find_element_by_name('ok')\n",
    "typeElem_btn.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "逐步登录成功。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "————————————合并成一个完整程序————————————————————"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_*_coding:utf-8_*_\n",
    "from selenium import webdriver\n",
    "import getpass\n",
    "\n",
    "path = '/Users/caimeijuan/anaconda3/lib/python3.7/site-packages/selenium/webdriver/chrome/chromedriver'\n",
    "browser = webdriver.Chrome(path)\n",
    "\n",
    "def gmailSignin(email, password):\n",
    "    #打开登录界面\n",
    "    url = 'http://gmail.com'\n",
    "    browser.get(url)\n",
    "\n",
    "    # 填写email帐号\n",
    "    tagsElem_input = browser.find_elements_by_tag_name('input')\n",
    "    emailElem = tagsElem_input[0]\n",
    "    emailElem.clear()\n",
    "    emailElem.send_keys(email)\n",
    "\n",
    "    # 点击下一步\n",
    "    classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "    classElem_btn.click()\n",
    "\n",
    "    # 填写密码\n",
    "    Elem_input = browser.find_element_by_name('password')\n",
    "    print(Elem_input)\n",
    "    Elem_input.send_keys(password)\n",
    "\n",
    "    # 点击下一步\n",
    "    classElem_btn = browser.find_element_by_class_name('RveJvd')\n",
    "    classElem_btn.click()\n",
    "    \n",
    "email = getpass.getpass('Input your email account: ')\n",
    "password = getpass.getpass('Input your password: ') #不显示输入值\n",
    "gmailSignin(email, password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 获取动态网页数据\n",
    "\n",
    "记录了一堆教程：  \n",
    "\n",
    "- [Web crawler with Python - 04.另一种抓取方式 - 知乎](https://zhuanlan.zhihu.com/p/20430122)\n",
    "\n",
    "> 永远记住，对于爬虫程序，模拟浏览器往往是下下策，只有实在没有办法了，才去考虑模拟浏览器环境，因为那样的内存开销实在是很大，而且效率非常低。\n",
    "\n",
    "- [Python 网络爬虫：解析JSON, 获取JS动态内容—爬取今日头条, 抓取json内容 - Just Code](http://justcode.ikeepstudying.com/2018/12/python-%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB%EF%BC%9A%E8%A7%A3%E6%9E%90json-%E8%8E%B7%E5%8F%96js%E5%8A%A8%E6%80%81%E5%86%85%E5%AE%B9-%E7%88%AC%E5%8F%96%E4%BB%8A%E6%97%A5%E5%A4%B4%E6%9D%A1/)  \n",
    "- [用python抓取淘宝评论 - 云+社区 - 腾讯云](https://cloud.tencent.com/developer/article/1059747)  \n",
    "- [Python 从零开始爬虫(五)——初遇json&爬取某宝商品信息 - Python 从零开始爬虫 - SegmentFault 思否](https://segmentfault.com/a/1190000014688216)  \n",
    "- [Python爬取拉钩招聘网，让你清楚了解Python行业 - 掘金](https://juejin.im/post/5dc3ce0a6fb9a04aba52b643)\n",
    "- [Python搭建代理池爬取拉勾网招聘信息 - 掘金](https://juejin.im/post/5d5e92916fb9a06ac93cd5f5)\n",
    "\n",
    "### 5.1 拉勾网  \n",
    "\n",
    "#### 5.1.1 `requests`和`bs`的常规做法：  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, bs4, lxml, os\n",
    "\n",
    "url = 'https://www.lagou.com/zhaopin/Python/?labelWords=label'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text, 'lxml')\n",
    "soup.select('.list_item_top')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "结果理所当然为空。因为数据是js动态发送的。  \n",
    "\n",
    "#### 5.1.2 找json数据  \n",
    "\n",
    "在F12（option+command+I）下，以前都是看elements，现在改看network，观察其中的XHR部分，在页面刷新或者点击页面上的一些切换按钮（总之就是让页面产生变化）时，看这一部分新增加的有哪些。取网址读取其中的json数据进行分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': False,\n",
       " 'msg': '您操作太频繁,请稍后再访问',\n",
       " 'clientIp': '221.225.172.92',\n",
       " 'state': 2408}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests, json, lxml, os\n",
    "\n",
    "# 点击了职位选项后真的看到了positionAjax，通过右击copy link address或header里的url或双击打开取url，都能获得下面这个url。\n",
    "url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&needAddtionalResult=false'\n",
    "res = requests.post(url)\n",
    "res.raise_for_status()\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3  增加cookie\n",
    "\n",
    "上面的结果，和[Python搭建代理池爬取拉勾网招聘信息 - 掘金](https://juejin.im/post/5d5e92916fb9a06ac93cd5f5)的描述一样，没有获得真正的data数据。按照这篇文章的说法来寻找cookie。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'getCookie' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-9b0510705cd3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;34m\"Content-Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"application/x-www-form-urlencoded; charset=UTF-8\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;34m\"User-Agent\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUserAgent\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;34m\"Cookie\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgetCookie\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m }\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getCookie' is not defined"
     ]
    }
   ],
   "source": [
    "UserAgent = \"Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.25 Safari/537.36 Core/1.70.3704.400 QQBrowser/10.4.3587.400\"\n",
    "headers = {\n",
    "    \"Accept\": \"application/json, text/javascript, */*; q=0.01\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Host\": \"www.lagou.com\",\n",
    "    \"Referer\": 'https://www.lagou.com/jobs/list_Python?px=default&city=%E6%AD%A6%E6%B1%89',\n",
    "    \"Content-Type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"User-Agent\": UserAgent,\n",
    "    \"Cookie\": getCookie()\n",
    "}\n",
    "\n",
    "url = 'https://www.lagou.com/jobs/positionAjax.json?px=default&needAddtionalResult=false'\n",
    "res = requests.post(url, headers=headers)\n",
    "res.raise_for_status()\n",
    "res.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, json, bs4, lxml\n",
    "\n",
    "url = 'https://www.toutiao.com/api/pc/realtime_news/' # 右击copy link address\n",
    "res = requests.get(url)\n",
    "data = json.loads(res.text)\n",
    "\n",
    "for i in range(len(data['data'])) :\n",
    "    newsUrl = 'https://www.toutiao.com' + data['data'][i]['open_url']\n",
    "    # print(newsUrl)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, bs4, lxml, json\n",
    "\n",
    "url = 'https://item.taobao.com/item.htm?spm=a219r.lm874.14.13.292512d5Thbdic&id=599063101905&ns=1&abbucket=9'\n",
    "res = requests.get(url)\n",
    "res.raise_for_status()\n",
    "soup = bs4.BeautifulSoup(res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始链接\n",
    "url = 'https://s.taobao.com/search?\\\n",
    "       q=%E9%98%94%E8%85%BF%E8%A3%A4&\\\n",
    "       imgfile=&\\\n",
    "       js=1&\\\n",
    "       stats_click=search_radio_all%3A1&\\\n",
    "       initiative_id=staobaoz_20191109&\\\n",
    "       ie=utf8'\n",
    "\n",
    "# 有排序\n",
    "url = 'https://s.taobao.com/search?\\\n",
    "       q=%E9%98%94%E8%85%BF%E8%A3%A4&\\\n",
    "       imgfile=&\\\n",
    "       js=1&\\\n",
    "       stats_click=search_radio_all%3A1&\\\n",
    "       initiative_id=staobaoz_20191109&\\\n",
    "       ie=utf8&\\\n",
    "       sort=sale-desc'\n",
    "\n",
    "# 下一页\n",
    "url = 'https://s.taobao.com/search?\\\n",
    "       q=%E9%98%94%E8%85%BF%E8%A3%A4&\\\n",
    "       imgfile=&\\\n",
    "       js=1&\\\n",
    "       stats_click=search_radio_all%3A1&\\\n",
    "       initiative_id=staobaoz_20191109&\\\n",
    "       ie=utf8&\\\n",
    "       sort=sale-desc&\\\n",
    "       bcoffset=0&\\\n",
    "       p4ppushleft=%2C44&\\\n",
    "       s=44'\n",
    "url = 'https://s.taobao.com/search?\\\n",
    "       q=%E9%98%94%E8%85%BF%E8%A3%A4&\\\n",
    "       imgfile=&\\\n",
    "       js=1&\\\n",
    "       stats_click=search_radio_all%3A1&\\\n",
    "       initiative_id=staobaoz_20191109&\\\n",
    "       ie=utf8&\\\n",
    "       sort=sale-desc&\\\n",
    "       s=44'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Python format 格式化函数 | 菜鸟教程](https://www.runoob.com/python/att-string-format.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://s.taobao.com/search?\\\n",
    "       q={name}&\\\n",
    "       imgfile=&\\\n",
    "       js=1&\\\n",
    "       stats_click=search_radio_all%3A1&\\\n",
    "       initiative_id=staobaoz_{date}&\\\n",
    "       ie=utf8&\\\n",
    "       sort={sort}&\\\n",
    "       s={num}'.format(name='zara', date='20191109', sort='price-asc', num=88)\n",
    "# sort：默认排序（按综合排），default；\n",
    "# sort：按销量排，sale-desc；\n",
    "# sort：按信用拍，credit-desc；\n",
    "# sort：按价格从低到高，price-asc；\n",
    "# sort：按价格从高到低，price-desc；\n",
    "# sort：总价从低到高，total-asc；\n",
    "# sort：总价从高到低，total-desc；\n",
    "\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import bs4, requests, lxml, json\n",
    "url = 'https://www.smzdm.com/homepage/json_more?timesort=1573524781&p=2&past_num=20'\n",
    "url = 'https://www.smzdm.com/homepage/json_more?timesort=1573525077&p=3&past_num=40'\n",
    "res = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
